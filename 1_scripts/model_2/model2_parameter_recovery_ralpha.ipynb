{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "444d1187",
   "metadata": {},
   "source": [
    "this model will operate on two learning rates based on condition and fits r_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "530b91d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d292965",
   "metadata": {},
   "source": [
    "### Define environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b369fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    \n",
    "    \"\"\"Class for the RP learning task\n",
    "\n",
    "    Parameters \n",
    "    ----------\n",
    "\n",
    "    n_actions : array, float \n",
    "        choosing the top or bottom stimulus\n",
    "\n",
    "    r_p :\n",
    "        reward probability with 80/20 contingency\n",
    "\n",
    "    p_p :\n",
    "        punishment probability with 80/20 contingency\n",
    "\n",
    "    inv_rp :\n",
    "        inverse reward probability \n",
    "\n",
    "    inv_pp :\n",
    "        inverse punishment probability\n",
    "\n",
    "    best_action : \n",
    "        pre-defined action that's the 'best'\n",
    "        set as a np.random variable that's either 1 or 2 to randomize every time I initialize\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_actions, r_p, p_p, inv_rp, inv_pp, best_action):\n",
    "        \n",
    "        self.n_actions = n_actions # choice of top or bottom stimulus\n",
    "\n",
    "        self.r_p = r_p             # reward prob outcome\n",
    "        self.p_p = p_p             # punishment prob outcome\n",
    "\n",
    "        self.inv_rp = inv_rp       # inverse reward prob\n",
    "        self.inv_pp = inv_pp       # inverse punishment prob\n",
    "\n",
    "        self.best_action = best_action  # predefined best action\n",
    "\n",
    "# Step Function: based on the condition, the environment returns an appropriate reward\n",
    "\n",
    "    \"\"\"\n",
    "    Conditions are set such that 1 = Reward, 2=Punishment, 3=Neutral\n",
    "    The best action is that which is associated with a higher probability of returning reward\n",
    "        - Taking the best action returns a value of 1\n",
    "        - Not taking the best action returns a value of 0 \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def step(self, action, condition, trial):       # takes in condition  \n",
    "        \n",
    "        if condition == 1:                          ## Reward\n",
    "            if action == self.best_action:                               \n",
    "                reward = self.r_p[trial]            # index through r_p for 80% chance reward\n",
    "                took_best_action = 1                # true, best action was taken \n",
    "            else: \n",
    "                reward = self.inv_rp[trial]         # index through inv_rp for 20% chance reward\n",
    "                took_best_action = 0                # false \n",
    "                \n",
    "        elif condition == 2:                        ## Punishment\n",
    "            if action == self.best_action:\n",
    "                reward = self.p_p[trial]\n",
    "                took_best_action = 1                \n",
    "            else:\n",
    "                reward = self.inv_pp[trial] \n",
    "                took_best_action = 0\n",
    "        else:                                       ## Neutral\n",
    "            reward = 0\n",
    "            took_best_action = 3        \n",
    "\n",
    "        return reward, took_best_action, condition             # return condition agent's in to update "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae111376",
   "metadata": {},
   "source": [
    "### Define agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d66e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "\n",
    "    \"\"\" Class for the agent to operate on a soft max policy when choosing between the two actions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    r_alpha : float, range (0, 1)\n",
    "        reward learning rate \n",
    "    p_alpha : float, range (0, 1)\n",
    "        punishment learning rate\n",
    "    beta : float, range (0, inf) \n",
    "      inverse temperature to control level of stochasticity in the choice\n",
    "      **0 means the agent explores randomly \n",
    "      **large value approaching inf acts more deterministically\n",
    "      \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, r_alpha, p_alpha, beta, q_init= False):\n",
    "\n",
    "        # initialize action space which is an array of all possible actions\n",
    "        self.action_space = np.arange(env.n_actions) # 2 possible actions\n",
    "\n",
    "        # initialize parameters\n",
    "        self.r_alpha = r_alpha\n",
    "        self.p_alpha = p_alpha\n",
    "        self.beta = beta\n",
    "\n",
    "        # initialize Q-values \n",
    "        if q_init: \n",
    "            self.q = q_init # assigns q to those initial values     \n",
    "        else:   \n",
    "            self.q = np.zeros((3, env.n_actions)) # otherwise they are 6 values of 0 (2 stimuli per condition)\n",
    "\n",
    "        # initialize action counter, this counts how many times an action is taken\n",
    "        self.action_counter = np.zeros((env.n_actions, ))\n",
    "        \n",
    "    # Learning policy\n",
    "        \n",
    "    def soft_max_policy(self, condition):        \n",
    "        p = np.exp(self.beta * self.q[condition-1,:]) / (np.exp(self.beta * self.q[condition-1,:])).sum() # prob of choosing an action by condition        \n",
    "        action = np.nonzero(np.random.random((1,)) <= np.cumsum(p))[0][0] + 1               \n",
    "        return action # returns 1 or 2\n",
    "    \n",
    "    # Q-learning update function\n",
    "  \n",
    "    def update(self, condition, action, reward, verbose=False):\n",
    "        if condition == 1: # reward trial\n",
    "            self.action_counter[action-1] = self.action_counter[action-1] + 1        \n",
    "            self.q[condition-1, action-1] = self.q[condition-1, action-1] + self.r_alpha*(reward - self.q[condition-1, action-1]) # update by condition\n",
    "            # print('updated q values for reward trial' + str(self.q)) \n",
    "        elif condition == 2: # punishment trial \n",
    "            self.action_counter[action-1] = self.action_counter[action-1] + 1\n",
    "            self.q[condition-1, action-1] = self.q[condition-1, action-1] + self.p_alpha*(reward - self.q[condition-1, action-1]) # update by condition\n",
    "            # print('updated q values for punishment trial' + str(self.q)) \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48f3a85",
   "metadata": {},
   "source": [
    "### Define simulation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e9807d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RP_simulation(n_timesteps, n_trials_per_block, params, verbose=False):\n",
    "    \n",
    "    \"\"\"Function for running one simulation of the RL model \n",
    "    specifying how the environment and agent interact \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    n_timesteps :\n",
    "        how many timesteps to run the simulation for\n",
    "        \n",
    "    params : dictionary containing parameters of the simulation \n",
    "        \n",
    "        Environment parameters\n",
    "        ----------------------\n",
    "        n_actions: int \n",
    "            number of actions the agent can choose from \n",
    "        r_p : \n",
    "            possible returns from reward condition\n",
    "        p_p : \n",
    "            possible returns from punishment condition\n",
    "        inv_rp :\n",
    "            inverse reward probability\n",
    "        inv_pp :\n",
    "            inverse punishment probability\n",
    "        best_action: int\n",
    "            which is the best action\n",
    "            \n",
    "        Agent parameters\n",
    "        ----------------\n",
    "        r_alpha : \n",
    "            reward learning rate\n",
    "        p_alpha :\n",
    "            punishment learning rate\n",
    "        beta : \n",
    "            inverse temperature\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    sim_output: dictionary containing simulation output\n",
    "\n",
    "        actions: array, int, shape(n_timesteps, )\n",
    "            Action that the agent took on each timestep.\n",
    "\n",
    "        rewards: array, float, shape(n_timesteps, )\n",
    "            Rewards that the agent received on each timestep.\n",
    "\n",
    "        optimal_action: array, boolean, shape(n_timesteps, )\n",
    "            1 is true, 0 is false\n",
    "            \n",
    "        condition: \n",
    "            reward(1), punishment(2), neutral(3)\n",
    "        \n",
    "    \"\"\"\n",
    "    # initialize environment \n",
    "    env = Environment(params['n_actions'], params['r_p'], params['p_p'], params['inv_rp'], params['inv_pp'], params['best_action'])\n",
    "    \n",
    "    # initialize agent\n",
    "    agent = Agent(env, params['r_alpha'], params['p_alpha'], params['beta'])\n",
    "    \n",
    "    # initialize output lists \n",
    "    A = [] # action taken \n",
    "    R = [] # reward taken\n",
    "    OA = [] # was optimal action taken \n",
    "    C = [] # condition\n",
    "    \n",
    "    # Loop through trials\n",
    "    \n",
    "    a = np.tile([1], 30) # reward\n",
    "    b = np.tile([2], 30) # punishment \n",
    "    c = np.tile([3], 30) # neutral\n",
    "    d = np.concatenate([a,b,c])\n",
    "\n",
    "    np.random.shuffle(d) # shuffle order of conditions \n",
    "    e = np.array_split(d,3)\n",
    "        \n",
    "    for i in np.arange(n_timesteps): # 3\n",
    "        for t in np.arange(n_trials_per_block): # 30\n",
    "            \n",
    "            condition = e[i][t]\n",
    "        \n",
    "            # agent takes an action based on soft max policy which now takes in a condition parameter \n",
    "            action = agent.soft_max_policy(condition) \n",
    "            \n",
    "            # environment responds with a reward \n",
    "            reward, took_best_action, condition = env.step(action, condition, t)\n",
    "\n",
    "            # record action, reward, and optimal outcome result\n",
    "            A.append(action)\n",
    "            R.append(reward)\n",
    "            OA.append(took_best_action)\n",
    "            C.append(condition)\n",
    "\n",
    "            # update \n",
    "            agent.update(condition, action, reward)\n",
    "        \n",
    "    sim_output = {\n",
    "        'timestep': np.arange(n_timesteps)+1,\n",
    "        'actions': np.array(A),\n",
    "        'rewards': np.array(R),\n",
    "        'optimal_action': np.array(OA),\n",
    "        'condition': np.array(C)\n",
    "    }\n",
    "        \n",
    "    return env, agent, sim_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b061ebb8",
   "metadata": {},
   "source": [
    "### Parameter Recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70f74f6",
   "metadata": {},
   "source": [
    "##### Define likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f7d4565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T is the number of trials, 90 \n",
    "# L represents the log likelihood for all trials such that as you go through each trial you compute one value and that gets added on\n",
    "\n",
    "def m2_loglikelihood(training_params, p_alpha, beta, actions, rewards, condition):\n",
    "    \n",
    "    r_alpha = training_params\n",
    "\n",
    "    q_value = np.ones((3,2))*0.0 \n",
    "    T = len(actions)\n",
    "    L = 0  \n",
    "    \n",
    "    for t in range(T): \n",
    "        \n",
    "        # compute choice probabilities of picking an action based on soft max \n",
    "        p = np.exp(beta * q_value[condition[t]-1,:]) / (np.exp(beta * q_value[condition[t]-1,:])).sum()\n",
    "\n",
    "        # compute choice probability for actual choice based on the probability computed above by condition \n",
    "        choiceProb = p[actions[t]-1]\n",
    "\n",
    "        # sum of the natural log of each individual choice probability to get the prob of a whole dataset (90 trials)\n",
    "        L += np.log(choiceProb) \n",
    "    \n",
    "        # update values with q learning, index for t and update by condition using the two separate learning rates\n",
    "        if condition[t] == 1:\n",
    "            q_value[condition[t]-1, actions[t]-1] = q_value[condition[t]-1, actions[t]-1] + r_alpha * (rewards[t] - q_value[condition[t]-1, actions[t]-1])\n",
    "        elif condition[t] == 2:\n",
    "            q_value[condition[t]-1, actions[t]-1] = q_value[condition[t]-1, actions[t]-1] + p_alpha * (rewards[t] - q_value[condition[t]-1, actions[t]-1])\n",
    "    \n",
    "    return -L   # this will return a negative log likelihood which is what we want to minimize to perform parameter recovery/fitting on\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497cf73a",
   "metadata": {},
   "source": [
    "##### Define fitting function for minimizing likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1de7c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fitting r_alpha\n",
    "This fitting function returns a value for 'fun' which is the -LL & a value for 'x' which is the best fit alpha\n",
    "\n",
    "'''\n",
    "\n",
    "def fit_RP_Learning(p_alpha, beta, actions, rewards, condition):\n",
    "\n",
    "    init_cond = np.array([np.random.uniform(0,1)])    \n",
    "    bnds = [(1e-6,1)]\n",
    "    optimum_output = minimize(m2_loglikelihood, init_cond, args=(p_alpha, beta, actions, rewards, condition), method='L-BFGS-B', bounds=bnds)\n",
    "     \n",
    "    return optimum_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf120f7c",
   "metadata": {},
   "source": [
    "### Model Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f0f3810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2_BIC(actions, rewards, condition):\n",
    "    \n",
    "    init_cond = np.array([np.random.uniform(0,1)]) # reward alpha\n",
    "    bnds = [(1e-6,1)] \n",
    "    km = len(bnds) # number of parameters fit in the model\n",
    "\n",
    "    optimum_output = minimize(m2_loglikelihood, init_cond, args=(p_alpha, beta, actions, rewards, condition), method='L-BFGS-B', bounds=bnds)    \n",
    "    neg_loglikelihood = optimum_output.fun\n",
    "    \n",
    "    BIC = km * np.log(len(actions)) + 2*neg_loglikelihood\n",
    "    \n",
    "    return BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5855e98e",
   "metadata": {},
   "source": [
    "##### load model 1 functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "948cab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1_BIC(actions, rewards, condition):\n",
    "    \n",
    "    init_cond = [np.random.uniform(0,1)] # learning rate\n",
    "    bnds = [(1e-6,1)]\n",
    "    km = len(bnds) # number of parameters fit in the model\n",
    "    \n",
    "    optimum_output = minimize(m1_loglikelihood, init_cond, args=(beta, actions, rewards, condition), method='L-BFGS-B',bounds=bnds)    \n",
    "    neg_loglikelihood = optimum_output.fun\n",
    "    \n",
    "    BIC = km * np.log(len(actions)) + 2*neg_loglikelihood\n",
    "    return BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cc6fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m1_loglikelihood(alpha, beta, actions, reward, condition):\n",
    "    \n",
    "    q_value = np.ones((3,2))*0.0\n",
    "    T = len(actions)\n",
    "    L = 0  \n",
    "    \n",
    "    for t in range(T): # for every trial \n",
    "        \n",
    "        # compute choice probabilities of picking an action based on soft max \n",
    "        p = np.exp(beta * q_value[condition[t]-1,:]) / (np.exp(beta * q_value[condition[t]-1,:])).sum()\n",
    "\n",
    "        # compute choice probability for actual choice based on the probability computed above by condition \n",
    "        choiceProb = p[actions[t]-1]\n",
    " \n",
    "        # sum of the natural log of each individual choice probability to get the prob of a whole dataset (90 trials)\n",
    "        L += np.log(choiceProb) \n",
    "        \n",
    "        # update values with q learning, index for t and update by condition \n",
    "        if condition[t] == 1 or condition[t] == 2:\n",
    "            q_value[condition[t]-1, actions[t]-1] = q_value[condition[t]-1, actions[t]-1] + alpha * (reward[t] - q_value[condition[t]-1, actions[t]-1])\n",
    "\n",
    "    return -L   # this will return a negative log likelihood which is what we want to minimize to perform parameter recovery/fitting on\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bd9e61",
   "metadata": {},
   "source": [
    "##### Perform recovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d28d3da6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is simulation number 1\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.7777843903350473\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 1.0\n",
      "likelihood per trial: 0.6073042179635476\n",
      "done running this simulation\n",
      "this is simulation number 2\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.6044306646748021\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.34284825852341994\n",
      "likelihood per trial: 0.6698613714214443\n",
      "done running this simulation\n",
      "this is simulation number 3\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.008181023711492208\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.04018895352005406\n",
      "likelihood per trial: 0.5360133124456076\n",
      "done running this simulation\n",
      "this is simulation number 4\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.3685056163969409\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.4891426906622943\n",
      "likelihood per trial: 0.6077437827939773\n",
      "done running this simulation\n",
      "this is simulation number 5\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.14820882582659334\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.08145965641796597\n",
      "likelihood per trial: 0.564947194698292\n",
      "done running this simulation\n",
      "this is simulation number 6\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.5322703931741329\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.5963648072317927\n",
      "likelihood per trial: 0.649215497535122\n",
      "done running this simulation\n",
      "this is simulation number 7\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.21598864916762905\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.15558644668046034\n",
      "likelihood per trial: 0.648230003711991\n",
      "done running this simulation\n",
      "this is simulation number 8\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.959862993137162\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.8082040017257179\n",
      "likelihood per trial: 0.608037933484747\n",
      "done running this simulation\n",
      "this is simulation number 9\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.2585100877961938\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.3912941896919735\n",
      "likelihood per trial: 0.6938322225746393\n",
      "done running this simulation\n",
      "this is simulation number 10\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.7614414144417706\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.6837061604106832\n",
      "likelihood per trial: 0.6505378996271822\n",
      "done running this simulation\n",
      "this is simulation number 11\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.40724759063228944\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.39930844120283804\n",
      "likelihood per trial: 0.7036821968591677\n",
      "done running this simulation\n",
      "this is simulation number 12\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.23116665671994563\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.1781504720159862\n",
      "likelihood per trial: 0.6167975422639901\n",
      "done running this simulation\n",
      "this is simulation number 13\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.8392886479598941\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.47025463285604613\n",
      "likelihood per trial: 0.6447452661660196\n",
      "done running this simulation\n",
      "this is simulation number 14\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.5610253975443695\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.7817617274833327\n",
      "likelihood per trial: 0.65823545785578\n",
      "done running this simulation\n",
      "this is simulation number 15\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.9669736634280383\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.8093333624950821\n",
      "likelihood per trial: 0.65471925434184\n",
      "done running this simulation\n",
      "this is simulation number 16\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.4408003656886401\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.47951670371131555\n",
      "likelihood per trial: 0.6385570430593318\n",
      "done running this simulation\n",
      "this is simulation number 17\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.9630863432367537\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.15442596636610387\n",
      "likelihood per trial: 0.6160334632027763\n",
      "done running this simulation\n",
      "this is simulation number 18\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.7413383563694015\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.13528964829094817\n",
      "likelihood per trial: 0.559294152819372\n",
      "done running this simulation\n",
      "this is simulation number 19\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.9067836780743426\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 1.0\n",
      "likelihood per trial: 0.6273179023319275\n",
      "done running this simulation\n",
      "this is simulation number 20\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.04196949583496201\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.023409607747111795\n",
      "likelihood per trial: 0.5846835530405479\n",
      "done running this simulation\n",
      "this is simulation number 21\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.7455968552005748\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.7074610101237732\n",
      "likelihood per trial: 0.6954760111467937\n",
      "done running this simulation\n",
      "this is simulation number 22\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.8997302262494166\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.5078391922210398\n",
      "likelihood per trial: 0.6275566383981389\n",
      "done running this simulation\n",
      "this is simulation number 23\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.6488117793934561\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.7437810042743629\n",
      "likelihood per trial: 0.6699718695979274\n",
      "done running this simulation\n",
      "this is simulation number 24\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.2369405707429727\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.15186663251432111\n",
      "likelihood per trial: 0.6219149137250841\n",
      "done running this simulation\n",
      "this is simulation number 25\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.7924282453350951\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.9152977700127278\n",
      "likelihood per trial: 0.5947978399779684\n",
      "done running this simulation\n",
      "this is simulation number 26\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.3732426134696726\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.36242632137593295\n",
      "likelihood per trial: 0.6609569827464128\n",
      "done running this simulation\n",
      "this is simulation number 27\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.6308799436311685\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.35646558830945824\n",
      "likelihood per trial: 0.6342696804536313\n",
      "done running this simulation\n",
      "this is simulation number 28\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.031026869263410806\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.005002396866463795\n",
      "likelihood per trial: 0.5193552920557672\n",
      "done running this simulation\n",
      "this is simulation number 29\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.41011012320245177\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.4374922979961866\n",
      "likelihood per trial: 0.6872263911161195\n",
      "done running this simulation\n",
      "this is simulation number 30\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.6118498841010669\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.78729313995789\n",
      "likelihood per trial: 0.6870145063908725\n",
      "done running this simulation\n",
      "this is simulation number 31\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.4053154052486353\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.4020206408550201\n",
      "likelihood per trial: 0.6709615816097754\n",
      "done running this simulation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is simulation number 32\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.024762747689534015\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.001990521371376275\n",
      "likelihood per trial: 0.5531796337301302\n",
      "done running this simulation\n",
      "this is simulation number 33\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.8134106778342549\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.7683293687202928\n",
      "likelihood per trial: 0.6227631767496924\n",
      "done running this simulation\n",
      "this is simulation number 34\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.5813480276656876\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.4681293134059095\n",
      "likelihood per trial: 0.6074337625459654\n",
      "done running this simulation\n",
      "this is simulation number 35\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.6348767729847057\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.7734860573418102\n",
      "likelihood per trial: 0.6182862431178728\n",
      "done running this simulation\n",
      "this is simulation number 36\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.044259135274386074\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.041532171056012035\n",
      "likelihood per trial: 0.5634618377858387\n",
      "done running this simulation\n",
      "this is simulation number 37\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.455127146041588\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.3620335285016084\n",
      "likelihood per trial: 0.7077664974727589\n",
      "done running this simulation\n",
      "this is simulation number 38\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.3470897001339701\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.3795793450709999\n",
      "likelihood per trial: 0.6645660108095861\n",
      "done running this simulation\n",
      "this is simulation number 39\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.2452844831257054\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.424654279931023\n",
      "likelihood per trial: 0.7052478490176551\n",
      "done running this simulation\n",
      "this is simulation number 40\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.04801640021736098\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.05533441631694497\n",
      "likelihood per trial: 0.643704240729436\n",
      "done running this simulation\n",
      "this is simulation number 41\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.638990937103682\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.4237380025981271\n",
      "likelihood per trial: 0.6834476319013431\n",
      "done running this simulation\n",
      "this is simulation number 42\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.9627200295849523\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.8224014991023774\n",
      "likelihood per trial: 0.6068746519795591\n",
      "done running this simulation\n",
      "this is simulation number 43\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.933606562815159\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.8754039200451968\n",
      "likelihood per trial: 0.6546582831106076\n",
      "done running this simulation\n",
      "this is simulation number 44\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.3789549105576281\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.4311067473594882\n",
      "likelihood per trial: 0.6244493397295025\n",
      "done running this simulation\n",
      "this is simulation number 45\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.9216082944803335\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 1.0\n",
      "likelihood per trial: 0.5779692920279147\n",
      "done running this simulation\n",
      "this is simulation number 46\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.8557288798155421\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.0602332303502554\n",
      "likelihood per trial: 0.6024469920361385\n",
      "done running this simulation\n",
      "this is simulation number 47\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.874472117215242\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.821446178267518\n",
      "likelihood per trial: 0.5926834256335533\n",
      "done running this simulation\n",
      "this is simulation number 48\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.20333414282491846\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.3216593872871983\n",
      "likelihood per trial: 0.6003109905771509\n",
      "done running this simulation\n",
      "this is simulation number 49\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.3962618179281303\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.22790782967497408\n",
      "likelihood per trial: 0.5408918044388624\n",
      "done running this simulation\n",
      "this is simulation number 50\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.09931172901828489\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.07861118274244713\n",
      "likelihood per trial: 0.6373983026404489\n",
      "done running this simulation\n",
      "this is simulation number 51\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.3951016589150639\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.3568162997010614\n",
      "likelihood per trial: 0.6536112292472609\n",
      "done running this simulation\n",
      "this is simulation number 52\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.017729228826475207\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.011494190968528217\n",
      "likelihood per trial: 0.5739868041627972\n",
      "done running this simulation\n",
      "this is simulation number 53\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.48162239389812134\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.7088882454688845\n",
      "likelihood per trial: 0.6103611535662301\n",
      "done running this simulation\n",
      "this is simulation number 54\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.41979936209702307\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.8313308360983059\n",
      "likelihood per trial: 0.6407636320521497\n",
      "done running this simulation\n",
      "this is simulation number 55\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.7872228265070604\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.7676586058727563\n",
      "likelihood per trial: 0.6740971311513346\n",
      "done running this simulation\n",
      "this is simulation number 56\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.09467282435171998\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.101289502261102\n",
      "likelihood per trial: 0.5913864415848868\n",
      "done running this simulation\n",
      "this is simulation number 57\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.9169965607319325\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 1.0\n",
      "likelihood per trial: 0.6604569633291701\n",
      "done running this simulation\n",
      "this is simulation number 58\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.9675648770464557\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 1.0\n",
      "likelihood per trial: 0.671703419914615\n",
      "done running this simulation\n",
      "this is simulation number 59\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.4249511588894245\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.3102133008763705\n",
      "likelihood per trial: 0.6695354353388179\n",
      "done running this simulation\n",
      "this is simulation number 60\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.8503306612930838\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.5548404053938548\n",
      "likelihood per trial: 0.6630233751026884\n",
      "done running this simulation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is simulation number 61\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.522786980751362\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.586442464063181\n",
      "likelihood per trial: 0.627112414025549\n",
      "done running this simulation\n",
      "this is simulation number 62\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.9182345087193552\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.9337132863217912\n",
      "likelihood per trial: 0.6519918890008234\n",
      "done running this simulation\n",
      "this is simulation number 63\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.6290262406830496\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.4234145145430001\n",
      "likelihood per trial: 0.6986685485308216\n",
      "done running this simulation\n",
      "this is simulation number 64\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.729505648168097\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.6432704294348864\n",
      "likelihood per trial: 0.5989843875247256\n",
      "done running this simulation\n",
      "this is simulation number 65\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.7223271468952388\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.8639449152662838\n",
      "likelihood per trial: 0.6459070038837745\n",
      "done running this simulation\n",
      "this is simulation number 66\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.8112119219858187\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 1.0\n",
      "likelihood per trial: 0.6849735001790398\n",
      "done running this simulation\n",
      "this is simulation number 67\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.42782796073860485\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.6181582859062201\n",
      "likelihood per trial: 0.7101745223581264\n",
      "done running this simulation\n",
      "this is simulation number 68\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.9150929915308373\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.9177701174869455\n",
      "likelihood per trial: 0.6232355253520738\n",
      "done running this simulation\n",
      "this is simulation number 69\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.07666327244273319\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.9492309750011522\n",
      "likelihood per trial: 0.643182112119544\n",
      "done running this simulation\n",
      "this is simulation number 70\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.2986392548084946\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.2846577834644553\n",
      "likelihood per trial: 0.5800285450324906\n",
      "done running this simulation\n",
      "this is simulation number 71\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.7668865753652704\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.6857215384931159\n",
      "likelihood per trial: 0.6329500079317624\n",
      "done running this simulation\n",
      "this is simulation number 72\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.5827794906856962\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.382337723867986\n",
      "likelihood per trial: 0.6909691991093094\n",
      "done running this simulation\n",
      "this is simulation number 73\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.541078679769195\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.8026723039151765\n",
      "likelihood per trial: 0.6838423371113641\n",
      "done running this simulation\n",
      "this is simulation number 74\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.027599481949953875\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.024855502047529628\n",
      "likelihood per trial: 0.5818887459559806\n",
      "done running this simulation\n",
      "this is simulation number 75\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.715127014069377\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.5929513302273041\n",
      "likelihood per trial: 0.6718414279921553\n",
      "done running this simulation\n",
      "this is simulation number 76\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.24196091265727182\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.3132916510883579\n",
      "likelihood per trial: 0.6865440651551472\n",
      "done running this simulation\n",
      "this is simulation number 77\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.912655603667806\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.02126444163803922\n",
      "likelihood per trial: 0.5371856283774045\n",
      "done running this simulation\n",
      "this is simulation number 78\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.741372103439168\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.4390884246340823\n",
      "likelihood per trial: 0.6749715373974332\n",
      "done running this simulation\n",
      "this is simulation number 79\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.9097889493366211\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 1.0\n",
      "likelihood per trial: 0.6886413010113248\n",
      "done running this simulation\n",
      "this is simulation number 80\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.23957268220804573\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.37890620274316\n",
      "likelihood per trial: 0.6262884415711384\n",
      "done running this simulation\n",
      "this is simulation number 81\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.09602206666882274\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.05213439731481307\n",
      "likelihood per trial: 0.6070067488672954\n",
      "done running this simulation\n",
      "this is simulation number 82\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.32398287560148475\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.42072324048650805\n",
      "likelihood per trial: 0.6956075978610309\n",
      "done running this simulation\n",
      "this is simulation number 83\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.6130156014527228\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.6980108291463807\n",
      "likelihood per trial: 0.657318473008368\n",
      "done running this simulation\n",
      "this is simulation number 84\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.05905893304112764\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.0885775942917939\n",
      "likelihood per trial: 0.6431045220102833\n",
      "done running this simulation\n",
      "this is simulation number 85\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.26068882721769593\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.3072884466667621\n",
      "likelihood per trial: 0.6694475924569631\n",
      "done running this simulation\n",
      "this is simulation number 86\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.6149632843309033\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.4072818992767758\n",
      "likelihood per trial: 0.6770322499696352\n",
      "done running this simulation\n",
      "this is simulation number 87\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.3998972627206062\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.10107767603390845\n",
      "likelihood per trial: 0.6425653780748564\n",
      "done running this simulation\n",
      "this is simulation number 88\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.9180349617848372\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.7645863549251697\n",
      "likelihood per trial: 0.6384622096896753\n",
      "done running this simulation\n",
      "this is simulation number 89\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.44906070881227533\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.6841575032127216\n",
      "likelihood per trial: 0.5684162136826841\n",
      "done running this simulation\n",
      "this is simulation number 90\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.13699435505871882\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.09325862541480814\n",
      "likelihood per trial: 0.5612714117427031\n",
      "done running this simulation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is simulation number 91\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.21776146596362955\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.05801411659281792\n",
      "likelihood per trial: 0.5796995974090962\n",
      "done running this simulation\n",
      "this is simulation number 92\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.24856293990634803\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.3425370204069083\n",
      "likelihood per trial: 0.674834545219878\n",
      "done running this simulation\n",
      "this is simulation number 93\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.8322705044962977\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 1.0\n",
      "likelihood per trial: 0.6322878811754936\n",
      "done running this simulation\n",
      "this is simulation number 94\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.8679797041536528\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 1.0\n",
      "likelihood per trial: 0.5992503010675629\n",
      "done running this simulation\n",
      "this is simulation number 95\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.529286286435011\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.4542042628332042\n",
      "likelihood per trial: 0.673084003203529\n",
      "done running this simulation\n",
      "this is simulation number 96\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.4637840905318834\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.09435058827898013\n",
      "likelihood per trial: 0.5869370820708284\n",
      "done running this simulation\n",
      "this is simulation number 97\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.40244498817586494\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.4880130720698459\n",
      "likelihood per trial: 0.5975829801609378\n",
      "done running this simulation\n",
      "this is simulation number 98\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.8456712133928705\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.8025287048038448\n",
      "likelihood per trial: 0.6323504865968681\n",
      "done running this simulation\n",
      "this is simulation number 99\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.3859115943452568\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.8540697408539014\n",
      "likelihood per trial: 0.6324990159457534\n",
      "done running this simulation\n",
      "this is simulation number 100\n",
      "beta for this simulation is: 5\n",
      "p_alpha for this simulation is, 0.3\n",
      "r_alpha for this simulation is, 0.5720674846076961\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit reward alpha 0.7460965937885068\n",
      "likelihood per trial: 0.6308245237762725\n",
      "done running this simulation\n"
     ]
    }
   ],
   "source": [
    "# define parameters \n",
    "r_p = np.full((3, 8), 1, dtype=int)\n",
    "r_p = np.append(r_p, [0, 0, 0, 0, 0, 0])\n",
    "r_p = np.random.permutation(r_p)\n",
    "\n",
    "p_p = np.full((3, 8), 0, dtype=int)\n",
    "p_p = np.append(p_p, [-1, -1, -1, -1, -1, -1])\n",
    "p_p = np.random.permutation(p_p)\n",
    "\n",
    "inv_rp = np.full((24), 0, dtype=int)\n",
    "inv_rp = np.append(inv_rp, [1, 1, 1, 1, 1, 1])\n",
    "inv_rp= np.random.permutation(inv_rp)\n",
    "\n",
    "inv_pp = np.full((24), -1, dtype=int)\n",
    "inv_pp = np.append(inv_pp, [0, 0, 0, 0, 0, 0])\n",
    "inv_pp = np.random.permutation(inv_pp)\n",
    "\n",
    "n_timesteps = 3 # define how many times to loop through-- default to 3\n",
    "n_trials_per_block = 30 # default to 30\n",
    "\n",
    "best_action = np.random.choice(2) + 1   # action 1 or 2\n",
    "\n",
    "### Initialize output list to simulate through different values of alpha\n",
    "D = []\n",
    "\n",
    "for i in np.arange(1,101): # number of simulations to run \n",
    "    print('this is simulation number ' + str(i))\n",
    "\n",
    "    b = 5\n",
    "    p_alpha = 0.3\n",
    "    print('beta for this simulation is: ' + str(b))\n",
    "    print('p_alpha for this simulation is, ' + str(p_alpha))\n",
    "   \n",
    "    for r_alpha in np.random.uniform(0,1,1):  # define how many values of alpha to simulate for\n",
    "        print('r_alpha for this simulation is, ' + str(r_alpha))\n",
    "                    \n",
    "        print('best action = ' + str(best_action))\n",
    "\n",
    "        for i in np.arange(n_timesteps):\n",
    "\n",
    "            params = {\n",
    "            'n_actions' : 2,\n",
    "            'r_p': r_p,\n",
    "            'p_p': p_p,\n",
    "            'inv_rp': inv_rp,\n",
    "            'inv_pp': inv_pp,\n",
    "            'best_action' : best_action,\n",
    "            'r_alpha' : r_alpha,\n",
    "            'p_alpha' : p_alpha, \n",
    "            'beta' : b  \n",
    "            }\n",
    "\n",
    "        ### First, simulate with fixed parameter value. \n",
    "        _, _, sim_output = RP_simulation(n_timesteps, n_trials_per_block, params) # this returns actions, rewards, conditon, & optimal action cols\n",
    "\n",
    "        # Convert to dataframe and append alpha, beta, rewards, & optimal action    \n",
    "        d=pd.DataFrame(sim_output['actions'], columns = ['actions'])\n",
    "        d.insert(1, 'r_alpha', r_alpha),\n",
    "        d.insert(2, 'p_alpha', p_alpha),\n",
    "        d.insert(3, 'beta', b),\n",
    "        d.insert(4, 'rewards', sim_output['rewards']),\n",
    "        d.insert(5, 'condition', sim_output['condition'])\n",
    "        d.insert(6, 'optimal_action', sim_output['optimal_action']),\n",
    "        d.insert(7, 'best_action', best_action)\n",
    "\n",
    "        ### Then, fit simulated dataset to recover parameters. \n",
    "        ### PARAMETER RECOVERY\n",
    "        \n",
    "        print('Checking simulated dataset...')\n",
    "        \n",
    "        #  Obtain Log Likelihood function parameters (alpha, actions, rewards) using d   \n",
    "        sim_r_alpha = d[\"r_alpha\"].iloc[0]\n",
    "    \n",
    "        sim_p_alpha = d[\"p_alpha\"].iloc[0]\n",
    "        \n",
    "        beta = d[\"beta\"].iloc[0]\n",
    "        \n",
    "        actions = d['actions']\n",
    "        \n",
    "        rewards = d['rewards'].values\n",
    "    \n",
    "        condition = d[\"condition\"].values\n",
    "                \n",
    "        ### Then, compute best fit alpha which comes from fitting function and is fit_alpha.x[0]\n",
    "        \n",
    "        res = fit_RP_Learning(sim_p_alpha, beta, actions, rewards, condition) # optimizer to return optimal outputs\n",
    "\n",
    "        fit_r_alpha = res.x[0]\n",
    "        print('fit reward alpha ' + str(fit_r_alpha))\n",
    "\n",
    "        ### max LL gets computed with best fit parameters; alpha = fit_alpha.x[0]        \n",
    "\n",
    "        max_LL = m2_loglikelihood(fit_r_alpha, sim_p_alpha, beta, actions, rewards, condition) # this is already returned as a negative in my fx\n",
    "    \n",
    "        LL_per_trial = np.exp(-max_LL/len(actions))\n",
    "        print('likelihood per trial: ' + str(LL_per_trial))\n",
    "\n",
    "        print('done running this simulation')\n",
    "        \n",
    "        BIC_model1 = model1_BIC(actions, rewards, condition) \n",
    "        BIC_model2 = model2_BIC(actions, rewards, condition)\n",
    "\n",
    "        d.insert(8, 'log_likelihood', -max_LL),\n",
    "        d.insert(9, 'fit_r_alpha', res.x[0]),\n",
    "        d.insert(10, 'LL_per_trial', LL_per_trial)\n",
    "        d.insert(11, 'model1_BIC', BIC_model1)\n",
    "        d.insert(12, 'model2_BIC', BIC_model2)\n",
    "\n",
    "\n",
    "        D.append(d)\n",
    "\n",
    "        data = pd.concat(D, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9637fe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actions</th>\n",
       "      <th>r_alpha</th>\n",
       "      <th>p_alpha</th>\n",
       "      <th>beta</th>\n",
       "      <th>rewards</th>\n",
       "      <th>condition</th>\n",
       "      <th>optimal_action</th>\n",
       "      <th>best_action</th>\n",
       "      <th>log_likelihood</th>\n",
       "      <th>fit_r_alpha</th>\n",
       "      <th>LL_per_trial</th>\n",
       "      <th>model1_BIC</th>\n",
       "      <th>model2_BIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.777784</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-44.885289</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.607304</td>\n",
       "      <td>110.263078</td>\n",
       "      <td>94.270387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.777784</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-44.885289</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.607304</td>\n",
       "      <td>110.263078</td>\n",
       "      <td>94.270387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.777784</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-44.885289</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.607304</td>\n",
       "      <td>110.263078</td>\n",
       "      <td>94.270387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.777784</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-44.885289</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.607304</td>\n",
       "      <td>110.263078</td>\n",
       "      <td>94.270387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.777784</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-44.885289</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.607304</td>\n",
       "      <td>110.263078</td>\n",
       "      <td>94.270387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>1</td>\n",
       "      <td>0.572067</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-41.465479</td>\n",
       "      <td>0.746097</td>\n",
       "      <td>0.630825</td>\n",
       "      <td>89.683616</td>\n",
       "      <td>87.430768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>1</td>\n",
       "      <td>0.572067</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-41.465479</td>\n",
       "      <td>0.746097</td>\n",
       "      <td>0.630825</td>\n",
       "      <td>89.683616</td>\n",
       "      <td>87.430768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>1</td>\n",
       "      <td>0.572067</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-41.465479</td>\n",
       "      <td>0.746097</td>\n",
       "      <td>0.630825</td>\n",
       "      <td>89.683616</td>\n",
       "      <td>87.430768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>1</td>\n",
       "      <td>0.572067</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-41.465479</td>\n",
       "      <td>0.746097</td>\n",
       "      <td>0.630825</td>\n",
       "      <td>89.683616</td>\n",
       "      <td>87.430768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>2</td>\n",
       "      <td>0.572067</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-41.465479</td>\n",
       "      <td>0.746097</td>\n",
       "      <td>0.630825</td>\n",
       "      <td>89.683616</td>\n",
       "      <td>87.430768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      actions   r_alpha  p_alpha  beta  rewards  condition  optimal_action  \\\n",
       "0           1  0.777784      0.3     5        0          2               1   \n",
       "1           1  0.777784      0.3     5        1          1               1   \n",
       "2           1  0.777784      0.3     5       -1          2               1   \n",
       "3           2  0.777784      0.3     5        0          3               3   \n",
       "4           1  0.777784      0.3     5        0          3               3   \n",
       "...       ...       ...      ...   ...      ...        ...             ...   \n",
       "8995        1  0.572067      0.3     5        0          3               3   \n",
       "8996        1  0.572067      0.3     5        0          3               3   \n",
       "8997        1  0.572067      0.3     5        0          2               1   \n",
       "8998        1  0.572067      0.3     5        1          1               1   \n",
       "8999        2  0.572067      0.3     5        0          3               3   \n",
       "\n",
       "      best_action  log_likelihood  fit_r_alpha  LL_per_trial  model1_BIC  \\\n",
       "0               1      -44.885289     1.000000      0.607304  110.263078   \n",
       "1               1      -44.885289     1.000000      0.607304  110.263078   \n",
       "2               1      -44.885289     1.000000      0.607304  110.263078   \n",
       "3               1      -44.885289     1.000000      0.607304  110.263078   \n",
       "4               1      -44.885289     1.000000      0.607304  110.263078   \n",
       "...           ...             ...          ...           ...         ...   \n",
       "8995            1      -41.465479     0.746097      0.630825   89.683616   \n",
       "8996            1      -41.465479     0.746097      0.630825   89.683616   \n",
       "8997            1      -41.465479     0.746097      0.630825   89.683616   \n",
       "8998            1      -41.465479     0.746097      0.630825   89.683616   \n",
       "8999            1      -41.465479     0.746097      0.630825   89.683616   \n",
       "\n",
       "      model2_BIC  \n",
       "0      94.270387  \n",
       "1      94.270387  \n",
       "2      94.270387  \n",
       "3      94.270387  \n",
       "4      94.270387  \n",
       "...          ...  \n",
       "8995   87.430768  \n",
       "8996   87.430768  \n",
       "8997   87.430768  \n",
       "8998   87.430768  \n",
       "8999   87.430768  \n",
       "\n",
       "[9000 rows x 13 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6fd2ce",
   "metadata": {},
   "source": [
    "### visualizing parameter recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8495231",
   "metadata": {},
   "source": [
    "#### reward alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3e9b925",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7032607434355458"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = ( 10 , 5 ))\n",
    "\n",
    "figure = sns.scatterplot(ax = ax, x = \"r_alpha\", y = \"fit_r_alpha\",\n",
    "                data = data, color = \"c\")\n",
    "\n",
    "# Set label for x-axis\n",
    "ax.set_xlabel( \"Simulated Reward Alpha \" + \"(p_alpha= \" + str(p_alpha) + \")\" , size = 15 )\n",
    "  \n",
    "# Set label for y-axis\n",
    "ax.set_ylabel( \"Fit Reward Alpha\" , size = 15 )\n",
    "  \n",
    "# Set title for plot\n",
    "ax.set_title( \"Model 2 Simulations\" , size = 20 )\n",
    "  \n",
    "# Display figure\n",
    "#plt.plot([1, 0], [1, 0], linewidth=2)\n",
    "plt.show()\n",
    "data['r_alpha'].corr(data['fit_r_alpha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2a463b",
   "metadata": {},
   "source": [
    "#### punishment alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c6c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = ( 10 , 5 ))\n",
    "\n",
    "figure = sns.scatterplot(ax = ax, x = \"p_alpha\", y = \"fit_p_alpha\",\n",
    "                data = data, color = \"c\")\n",
    "\n",
    "# Set label for x-axis\n",
    "ax.set_xlabel( \"Simulated Punishment Alpha \" + \"(r_alpha= \" + str(r_alpha) + \")\" , size = 15 )\n",
    "  \n",
    "# Set label for y-axis\n",
    "ax.set_ylabel( \"Fit Puishment Alpha\" , size = 15 )\n",
    "  \n",
    "# Set title for plot\n",
    "ax.set_title( \"Model 2 Simulations\" , size = 20 )\n",
    "  \n",
    "# Display figure\n",
    "#plt.plot([1, 0], [1, 0], linewidth=2)\n",
    "plt.show()\n",
    "data['p_alpha'].corr(data['fit_p_alpha'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
