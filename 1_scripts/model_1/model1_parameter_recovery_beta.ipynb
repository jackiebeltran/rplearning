{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0638f3a",
   "metadata": {},
   "source": [
    "this model updates q values for only reward/punishment trials and operates using a single LR. We perform parameter recovery while holding alpha constant (best fit around 0.21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58184c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9773607",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    \n",
    "    \"\"\"Class for the RP learning task\n",
    "\n",
    "    Parameters \n",
    "    ----------\n",
    "\n",
    "    n_actions : array, float \n",
    "        choosing the top or bottom stimulus\n",
    "\n",
    "    r_p :\n",
    "        reward probability with 80/20 contingency\n",
    "\n",
    "    p_p :\n",
    "        punishment probability with 80/20 contingency\n",
    "\n",
    "    inv_rp :\n",
    "        inverse reward probability \n",
    "\n",
    "    inv_pp :\n",
    "        inverse punishment probability\n",
    "\n",
    "    best_action : \n",
    "        pre-defined action that's the 'best'\n",
    "        set as a np.random variable that's either 1 or 2 to randomize every time I initialize\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_actions, r_p, p_p, inv_rp, inv_pp, best_action):\n",
    "        \n",
    "        self.n_actions = n_actions # choice of top or bottom stimulus\n",
    "\n",
    "        self.r_p = r_p             # reward prob outcome\n",
    "        self.p_p = p_p             # punishment prob outcome\n",
    "\n",
    "        self.inv_rp = inv_rp       # inverse reward prob\n",
    "        self.inv_pp = inv_pp       # inverse punishment prob\n",
    "\n",
    "        self.best_action = best_action  # predefined best action\n",
    "\n",
    "# Step Function: based on the condition, the environment returns an appropriate reward\n",
    "\n",
    "    \"\"\"\n",
    "    Conditions are set such that 1 = Reward, 2=Punishment, 3=Neutral\n",
    "    The best action is that which is associated with a higher probability of returning reward\n",
    "        - Taking the best action returns a value of 1\n",
    "        - Not taking the best action returns a value of 0 \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def step(self, action, condition, trial): # takes in condition  \n",
    "        \n",
    "        if condition == 1:                          ## Reward\n",
    "            if action == self.best_action:                               \n",
    "                reward = self.r_p[trial]            # index through r_p for 80% chance reward\n",
    "                took_best_action = 1                # true, best action was taken \n",
    "            else: \n",
    "                reward = self.inv_rp[trial]         # index through inv_rp for 20% chance reward\n",
    "                took_best_action = 0                # false \n",
    "                \n",
    "        elif condition == 2:                        ## Punishment\n",
    "            if action == self.best_action:\n",
    "                reward = self.p_p[trial]\n",
    "                took_best_action = 1                \n",
    "            else:\n",
    "                reward = self.inv_pp[trial] \n",
    "                took_best_action = 0\n",
    "        else:                                       ## Neutral\n",
    "            reward = 0\n",
    "            took_best_action = 3        \n",
    "\n",
    "        return reward, took_best_action, condition             # return condition agent's in to update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b943b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \"\"\" Class for the agent to operate on a soft max policy when choosing between the two actions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float, range (0, 1)\n",
    "        Learning rate \n",
    "    beta : float, range (0, inf) \n",
    "      inverse temperature to control level of stochasticity in the choice\n",
    "      **0 means the agent explores randomly \n",
    "      **large value approaching inf acts more deterministically\n",
    "      \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, alpha, beta, q_init= False):\n",
    "\n",
    "        # initialize action space which is an array of all possible actions\n",
    "        self.action_space = np.arange(env.n_actions) # 2 possible actions\n",
    "\n",
    "        # initialize parameters\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "        # initialize Q-values \n",
    "        if q_init: \n",
    "            self.q = q_init # assigns q to those initial values     \n",
    "        else:   \n",
    "            self.q = np.zeros((3, env.n_actions)) # otherwise they are 6 values of 0 (2 stimuli per condition)\n",
    "\n",
    "        # initialize action counter, this counts how many times an action is taken\n",
    "        self.action_counter = np.zeros((env.n_actions, ))\n",
    "        \n",
    "    # Learning policy\n",
    "        \n",
    "    def soft_max_policy(self, condition):        \n",
    "        p = np.exp(self.beta * self.q[condition-1,:]) / (np.exp(self.beta * self.q[condition-1,:])).sum() # prob of choosing an action by condition        \n",
    "        action = np.nonzero(np.random.random((1,)) <= np.cumsum(p))[0][0] + 1               \n",
    "        return action # returns 1 or 2\n",
    "    \n",
    "    # Q-learning update function by trial type (only reward or punishment)\n",
    "  \n",
    "    def update(self, condition, action, reward, verbose=False):\n",
    "        if condition == 1 or condition == 2:\n",
    "            self.action_counter[action-1] = self.action_counter[action-1] + 1        \n",
    "            self.q[condition-1, action-1] = self.q[condition-1, action-1] + self.alpha*(reward - self.q[condition-1, action-1]) # update by condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2265d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RP_simulation(n_timesteps, n_trials_per_block, params, verbose=False):\n",
    "    \n",
    "    \"\"\"Function for running one simulation of the RL model \n",
    "    specifying how the environment and agent interact \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    n_timesteps :\n",
    "        how many timesteps to run the simulation for\n",
    "        \n",
    "    params : dictionary containing parameters of the simulation \n",
    "        \n",
    "        Environment parameters\n",
    "        ----------------------\n",
    "        n_actions: int \n",
    "            number of actions the agent can choose from \n",
    "        r_p : \n",
    "            possible returns from reward condition\n",
    "        p_p : \n",
    "            possible returns from punishment condition\n",
    "        inv_rp :\n",
    "            inverse reward probability\n",
    "        inv_pp :\n",
    "            inverse punishment probability\n",
    "        best_action: int\n",
    "            which is the best action\n",
    "            \n",
    "        Agent parameters\n",
    "        ----------------\n",
    "        alpha : \n",
    "            learning rate \n",
    "        beta : \n",
    "            inverse temperature\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    sim_output: dictionary containing simulation output\n",
    "\n",
    "        actions: array, int, shape(n_timesteps, )\n",
    "            Action that the agent took on each timestep.\n",
    "\n",
    "        rewards: array, float, shape(n_timesteps, )\n",
    "            Rewards that the agent received on each timestep.\n",
    "\n",
    "        optimal_action: array, boolean, shape(n_timesteps, )\n",
    "            1 is true, 0 is false\n",
    "            \n",
    "        condition: \n",
    "            reward(1), punishment(2), neutral(3)\n",
    "        \n",
    "    \"\"\"\n",
    "    # initialize environment \n",
    "    env = Environment(params['n_actions'], params['r_p'], params['p_p'], params['inv_rp'], params['inv_pp'], params['best_action'])\n",
    "    \n",
    "    # initialize agent\n",
    "    agent = Agent(env, params['alpha'], params['beta'])\n",
    "    \n",
    "    # initialize output lists \n",
    "    A = [] # action taken \n",
    "    R = [] # reward taken\n",
    "    OA = [] # was optimal action taken \n",
    "    C = [] # condition\n",
    "    \n",
    "    # Loop through trials\n",
    "    \n",
    "    a = np.tile([1], 30) # reward\n",
    "    b = np.tile([2], 30) # punishment \n",
    "    c = np.tile([3], 30) # neutral\n",
    "    d = np.concatenate([a,b,c])\n",
    "\n",
    "    np.random.shuffle(d) # shuffle order of conditions\n",
    "    e = np.array_split(d,3)\n",
    "        \n",
    "    for i in np.arange(n_timesteps): # 3\n",
    "        for t in np.arange(n_trials_per_block): # 30\n",
    "            \n",
    "            condition = e[i][t]       \n",
    "        \n",
    "            # agent takes an action based on soft max policy\n",
    "            action = agent.soft_max_policy(condition) \n",
    "            \n",
    "            # environment responds with a reward \n",
    "            reward, took_best_action, condition = env.step(action, condition, t)\n",
    "\n",
    "            # record action, reward, and optimal outcome result\n",
    "            A.append(action)\n",
    "            R.append(reward)\n",
    "            OA.append(took_best_action)\n",
    "            C.append(condition)\n",
    "\n",
    "            # update \n",
    "            agent.update(condition, action, reward)\n",
    "        \n",
    "    sim_output = {\n",
    "        'timestep': np.arange(n_timesteps)+1,\n",
    "        'actions': np.array(A),\n",
    "        'rewards': np.array(R),\n",
    "        'optimal_action': np.array(OA),\n",
    "        'condition': np.array(C)\n",
    "    }\n",
    "        \n",
    "    return env, agent, sim_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d70e1e9",
   "metadata": {},
   "source": [
    "## Parameter Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dedbdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T is the number of trials, 90 \n",
    "# L represents the log likelihood for all trials such that as you go through each trial you compute one value and that gets added on\n",
    "\n",
    "def m1_loglikelihood(beta, alpha, actions, reward, condition):\n",
    "    \n",
    "    q_value = np.ones((3,2))*0.0\n",
    "    T = len(actions)\n",
    "    L = 0  \n",
    "    \n",
    "    for t in range(T): # for every trial \n",
    "        \n",
    "        # compute choice probabilities of picking an action based on soft max \n",
    "        p = np.exp(beta * q_value[condition[t]-1,:]) / (np.exp(beta * q_value[condition[t]-1,:])).sum()\n",
    "\n",
    "        # compute choice probability for actual choice based on the probability computed above by condition \n",
    "        choiceProb = p[actions[t]-1]\n",
    " \n",
    "        # sum of the natural log of each individual choice probability to get the prob of a whole dataset (90 trials)\n",
    "        L += np.log(choiceProb) \n",
    "        \n",
    "        # update values with q learning, index for t and update by condition \n",
    "        if condition[t] == 1 or condition[t] == 2:\n",
    "            q_value[condition[t]-1, actions[t]-1] = q_value[condition[t]-1, actions[t]-1] + alpha * (reward[t] - q_value[condition[t]-1, actions[t]-1])\n",
    "\n",
    "    return -L   # this will return a negative log likelihood which is what we want to minimize to perform parameter recovery/fitting on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "017ffe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fitting beta\n",
    "This fitting function returns a value for 'fun' which is the -LL & a value for 'x' which is the best fit beta\n",
    "\n",
    "'''\n",
    "\n",
    "def fit_RP_Learning(alpha, actions, rewards, condition):\n",
    "    \n",
    "    init_cond = np.array([np.random.randint(1,50)]) \n",
    "    \n",
    "    bnds = [(1,50)]\n",
    "        \n",
    "    optimum_output = minimize(m1_loglikelihood, init_cond, args=(alpha, actions, rewards, condition), method='L-BFGS-B',bounds=bnds)\n",
    "    \n",
    "    return optimum_output "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40725fe0",
   "metadata": {},
   "source": [
    "### Model Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaa581bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1_BIC(actions, rewards, condition):\n",
    "    \n",
    "    init_cond = [np.random.uniform(0,1)] # learning rate\n",
    "    bnds = [(1,50)]\n",
    "    km = len(bnds) # number of parameters fit in the model\n",
    "    \n",
    "    optimum_output = minimize(m1_loglikelihood, init_cond, args=(alpha, actions, rewards, condition), method='L-BFGS-B',bounds=bnds)    \n",
    "    neg_loglikelihood = optimum_output.fun\n",
    "    \n",
    "    BIC = km * np.log(len(actions)) + 2*neg_loglikelihood\n",
    "    return BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef17fca",
   "metadata": {},
   "source": [
    "##### load model 2 functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28d9d917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2_BIC(actions, rewards, condition):\n",
    "    \n",
    "    init_cond = np.array([np.random.uniform(0,1), # reward alpha\n",
    "                          np.random.uniform(0,1)]) # punishment alpha\n",
    "    bnds = [(1e-6,1), (1e-6,1)] \n",
    "    km = len(bnds) # number of parameters fit in the model\n",
    "\n",
    "    optimum_output = minimize(m2_loglikelihood, init_cond, args=(beta, actions, rewards, condition), method='L-BFGS-B', bounds=bnds)\n",
    "    \n",
    "    neg_loglikelihood = optimum_output.fun\n",
    "    \n",
    "    BIC = km * np.log(len(actions)) + 2*neg_loglikelihood\n",
    "    \n",
    "    return BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "127a1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m2_loglikelihood(training_params, beta, actions, rewards, condition):\n",
    "    \n",
    "    r_alpha = training_params[0]\n",
    "    p_alpha = training_params[1]\n",
    "\n",
    "    q_value = np.ones((3,2))*0.0  \n",
    "    T = len(actions)\n",
    "    L = 0  \n",
    "    \n",
    "    for t in range(T): # for every trial        \n",
    "        \n",
    "        # compute choice probabilities of picking an action based on soft max \n",
    "        p = np.exp(beta * q_value[condition[t]-1,:]) / (np.exp(beta * q_value[condition[t]-1,:])).sum()\n",
    "        #print('LL function probability is ' + str(p))\n",
    "\n",
    "        # compute choice probability for actual choice based on the probability computed above by condition \n",
    "        choiceProb = p[actions[t]-1]\n",
    "        #print('choice probability is ' + str(choiceProb))\n",
    "\n",
    "        # sum of the natural log of each individual choice probability to get the prob of a whole dataset (90 trials)\n",
    "        L += np.log(choiceProb) \n",
    "    \n",
    "        # update values with q learning, index for t and update by condition using the two separate learning rates\n",
    "        if condition[t] == 1:\n",
    "            q_value[condition[t]-1, actions[t]-1] = q_value[condition[t]-1, actions[t]-1] + r_alpha * (rewards[t] - q_value[condition[t]-1, actions[t]-1])\n",
    "        elif condition[t] == 2:\n",
    "            q_value[condition[t]-1, actions[t]-1] = q_value[condition[t]-1, actions[t]-1] + p_alpha * (rewards[t] - q_value[condition[t]-1, actions[t]-1])\n",
    "    \n",
    "    return -L   # this will return a negative log likelihood which is what we want to minimize to perform parameter recovery/fitting on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004a0ce5",
   "metadata": {},
   "source": [
    "## Running a Simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06a72205",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning simulation number 1\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 38.512808278223375\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 40.44814696715978\n",
      "loglikelihood is 24.10031352236438\n",
      "likelihood per trial: 0.7650751142913805\n",
      "done running this simulation\n",
      "beginning simulation number 2\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 31.418343725123293\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 16.079025348893943\n",
      "loglikelihood is 31.021863827798498\n",
      "likelihood per trial: 0.7084417948557552\n",
      "done running this simulation\n",
      "beginning simulation number 3\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 33.951552435453486\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 25.10718736130287\n",
      "likelihood per trial: 0.7565635466223553\n",
      "done running this simulation\n",
      "beginning simulation number 4\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 33.892934628094245\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 44.75990842333457\n",
      "loglikelihood is 30.048890974893265\n",
      "likelihood per trial: 0.7161421727872186\n",
      "done running this simulation\n",
      "beginning simulation number 5\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 14.437390199832754\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 13.282134572800866\n",
      "loglikelihood is 30.45929944358465\n",
      "likelihood per trial: 0.7128839316819121\n",
      "done running this simulation\n",
      "beginning simulation number 6\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 6.512552988614239\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 5.540665354122786\n",
      "loglikelihood is 39.80023650684009\n",
      "likelihood per trial: 0.6426051293978572\n",
      "done running this simulation\n",
      "beginning simulation number 7\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 40.99291141507085\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 23.59475955086815\n",
      "likelihood per trial: 0.769384838087009\n",
      "done running this simulation\n",
      "beginning simulation number 8\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 21.849419029822684\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 22.663277144589934\n",
      "loglikelihood is 29.51051779764762\n",
      "likelihood per trial: 0.7204389195693306\n",
      "done running this simulation\n",
      "beginning simulation number 9\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 28.646601056274633\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 23.478575730334807\n",
      "likelihood per trial: 0.7703787024537522\n",
      "done running this simulation\n",
      "beginning simulation number 10\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 29.23079723243518\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 24.42894425422725\n",
      "likelihood per trial: 0.7622865730057298\n",
      "done running this simulation\n",
      "beginning simulation number 11\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 3.207924195255818\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 2.9185532739288873\n",
      "loglikelihood is 51.45485580039128\n",
      "likelihood per trial: 0.5645532209652413\n",
      "done running this simulation\n",
      "beginning simulation number 12\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 25.97185296391018\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 33.181957240179756\n",
      "loglikelihood is 31.233537960856836\n",
      "likelihood per trial: 0.7067775438084793\n",
      "done running this simulation\n",
      "beginning simulation number 13\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 6.524121749662282\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 6.759748031058284\n",
      "loglikelihood is 33.8030136630037\n",
      "likelihood per trial: 0.686884556643673\n",
      "done running this simulation\n",
      "beginning simulation number 14\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 22.556403135249035\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 25.670195792792967\n",
      "loglikelihood is 29.88783521200471\n",
      "likelihood per trial: 0.7174248626256973\n",
      "done running this simulation\n",
      "beginning simulation number 15\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 4.781522412615506\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 6.014196251653402\n",
      "loglikelihood is 39.2053527027638\n",
      "likelihood per trial: 0.6468667022626657\n",
      "done running this simulation\n",
      "beginning simulation number 16\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 45.980435180841766\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 24.121953675276053\n",
      "likelihood per trial: 0.7648911770450765\n",
      "done running this simulation\n",
      "beginning simulation number 17\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 4.913798175149063\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 5.79367359530917\n",
      "loglikelihood is 44.753420579960824\n",
      "likelihood per trial: 0.6081946935223298\n",
      "done running this simulation\n",
      "beginning simulation number 18\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 30.71633628017497\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 22.328238195000832\n",
      "loglikelihood is 33.40107905918159\n",
      "likelihood per trial: 0.6899590019239984\n",
      "done running this simulation\n",
      "beginning simulation number 19\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 14.336679260587465\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 11.560915473699831\n",
      "loglikelihood is 30.50629651140233\n",
      "likelihood per trial: 0.7125117682549414\n",
      "done running this simulation\n",
      "beginning simulation number 20\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 29.736142894279247\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 23.156319104619687\n",
      "likelihood per trial: 0.7731420872870115\n",
      "done running this simulation\n",
      "beginning simulation number 21\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 23.125574323990314\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 23.853657173736696\n",
      "likelihood per trial: 0.7671747749931408\n",
      "done running this simulation\n",
      "beginning simulation number 22\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 32.816805611977244\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 25.817666827741675\n",
      "likelihood per trial: 0.7506145824445656\n",
      "done running this simulation\n",
      "beginning simulation number 23\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 26.7826197990282\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 30.592607329149196\n",
      "loglikelihood is 30.022263285605955\n",
      "likelihood per trial: 0.7163540842592165\n",
      "done running this simulation\n",
      "beginning simulation number 24\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 26.95269173595966\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 21.968407207530365\n",
      "loglikelihood is 28.15937517406852\n",
      "likelihood per trial: 0.7313362443650441\n",
      "done running this simulation\n",
      "beginning simulation number 25\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 37.7545539942285\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 24.927281097874175\n",
      "likelihood per trial: 0.7580773983006925\n",
      "done running this simulation\n",
      "beginning simulation number 26\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 42.030572648837364\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 33.08332219079587\n",
      "loglikelihood is 24.79427971927649\n",
      "likelihood per trial: 0.7591985080258541\n",
      "done running this simulation\n",
      "beginning simulation number 27\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 49.896214285671846\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 24.982360275796008\n",
      "likelihood per trial: 0.7576136037908417\n",
      "done running this simulation\n",
      "beginning simulation number 28\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 9.066695455910692\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 9.832008021332385\n",
      "loglikelihood is 37.33771010824623\n",
      "likelihood per trial: 0.6604304592016483\n",
      "done running this simulation\n",
      "beginning simulation number 29\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 41.92888230079562\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 24.004770956970596\n",
      "likelihood per trial: 0.7658877370947533\n",
      "done running this simulation\n",
      "beginning simulation number 30\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 20.20409822428975\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 45.43188258228382\n",
      "loglikelihood is 25.219027843970068\n",
      "likelihood per trial: 0.7556239701779199\n",
      "done running this simulation\n",
      "beginning simulation number 31\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 33.28119068071339\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 27.148974965679397\n",
      "likelihood per trial: 0.7395929753663506\n",
      "done running this simulation\n",
      "beginning simulation number 32\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 39.300168232352505\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 26.678241174085073\n",
      "likelihood per trial: 0.7434714584361193\n",
      "done running this simulation\n",
      "beginning simulation number 33\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 44.38678018009985\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 23.549557419704197\n",
      "likelihood per trial: 0.7697713555241363\n",
      "done running this simulation\n",
      "beginning simulation number 34\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 37.90146090118437\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 30.26888143671672\n",
      "loglikelihood is 28.85221386603134\n",
      "likelihood per trial: 0.7257278809361094\n",
      "done running this simulation\n",
      "beginning simulation number 35\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 7.793269528286008\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 12.180245417314243\n",
      "loglikelihood is 38.76654036298829\n",
      "likelihood per trial: 0.6500283267890647\n",
      "done running this simulation\n",
      "beginning simulation number 36\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 40.24527917905257\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 37.070307375955345\n",
      "loglikelihood is 26.701879612460324\n",
      "likelihood per trial: 0.7432762118084493\n",
      "done running this simulation\n",
      "beginning simulation number 37\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 8.514662461409902\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 6.188559172755275\n",
      "loglikelihood is 46.652479175417945\n",
      "likelihood per trial: 0.5954958377434263\n",
      "done running this simulation\n",
      "beginning simulation number 38\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 12.582164290668105\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 13.398854385594149\n",
      "loglikelihood is 31.126324079178605\n",
      "likelihood per trial: 0.7076200051058241\n",
      "done running this simulation\n",
      "beginning simulation number 39\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 13.85976610955478\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 42.59564457039629\n",
      "loglikelihood is 25.117573228845032\n",
      "likelihood per trial: 0.7564762453398669\n",
      "done running this simulation\n",
      "beginning simulation number 40\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 32.58016721038\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 28.617064098377583\n",
      "likelihood per trial: 0.7276265240262566\n",
      "done running this simulation\n",
      "beginning simulation number 41\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 34.656107907284884\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 26.412572645947602\n",
      "loglikelihood is 30.711971870543216\n",
      "likelihood per trial: 0.7108853372357133\n",
      "done running this simulation\n",
      "beginning simulation number 42\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 10.515625068253746\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 11.893692380360791\n",
      "loglikelihood is 28.823398684716025\n",
      "likelihood per trial: 0.7259602734751389\n",
      "done running this simulation\n",
      "beginning simulation number 43\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 21.015943876729672\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 16.301866345304447\n",
      "loglikelihood is 29.5487930302624\n",
      "likelihood per trial: 0.7201325961858155\n",
      "done running this simulation\n",
      "beginning simulation number 44\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 27.00129990984514\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 17.635754948047087\n",
      "loglikelihood is 29.63068981581819\n",
      "likelihood per trial: 0.7194775993006585\n",
      "done running this simulation\n",
      "beginning simulation number 45\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 7.843930573230464\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 6.5313618549125465\n",
      "loglikelihood is 39.19600537647307\n",
      "likelihood per trial: 0.646933888797493\n",
      "done running this simulation\n",
      "beginning simulation number 46\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 39.31026260556782\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 30.498794636388187\n",
      "loglikelihood is 28.110291096748664\n",
      "likelihood per trial: 0.7317352083124201\n",
      "done running this simulation\n",
      "beginning simulation number 47\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 24.086426832132997\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 14.803837559805345\n",
      "loglikelihood is 35.929826007015436\n",
      "likelihood per trial: 0.670842905784551\n",
      "done running this simulation\n",
      "beginning simulation number 48\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 27.16374666718457\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 20.25994541303361\n",
      "loglikelihood is 25.94894924762901\n",
      "likelihood per trial: 0.7495204639771695\n",
      "done running this simulation\n",
      "beginning simulation number 49\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 47.29789298835419\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 23.23213833467942\n",
      "likelihood per trial: 0.7724910389167179\n",
      "done running this simulation\n",
      "beginning simulation number 50\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 49.381733172001155\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 36.717584691266914\n",
      "loglikelihood is 25.628067249394224\n",
      "likelihood per trial: 0.7521975404415464\n",
      "done running this simulation\n",
      "beginning simulation number 51\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 25.102685279727158\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 24.57478035693883\n",
      "likelihood per trial: 0.7610523632004234\n",
      "done running this simulation\n",
      "beginning simulation number 52\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 15.67402357828915\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 10.012628876629718\n",
      "loglikelihood is 34.983867768538985\n",
      "likelihood per trial: 0.6779310842393719\n",
      "done running this simulation\n",
      "beginning simulation number 53\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 27.281345244967824\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 27.92616184706452\n",
      "loglikelihood is 31.063808605989855\n",
      "likelihood per trial: 0.7081117002942287\n",
      "done running this simulation\n",
      "beginning simulation number 54\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 20.396471956865916\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 20.07842386542477\n",
      "loglikelihood is 26.2594979222054\n",
      "likelihood per trial: 0.7469386698689354\n",
      "done running this simulation\n",
      "beginning simulation number 55\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 18.228899279882135\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 23.14728823915827\n",
      "likelihood per trial: 0.7732196705368886\n",
      "done running this simulation\n",
      "beginning simulation number 56\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 22.714727101385414\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 17.82759394226547\n",
      "loglikelihood is 30.87897537825987\n",
      "likelihood per trial: 0.709567445406935\n",
      "done running this simulation\n",
      "beginning simulation number 57\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 11.180316298257539\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 13.056456151153702\n",
      "loglikelihood is 31.414709358775553\n",
      "likelihood per trial: 0.7053562206875403\n",
      "done running this simulation\n",
      "beginning simulation number 58\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 7.137972345320924\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 8.95333089210749\n",
      "loglikelihood is 41.89482500023444\n",
      "likelihood per trial: 0.6278223369036825\n",
      "done running this simulation\n",
      "beginning simulation number 59\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 29.350554503036694\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 22.56395815306522\n",
      "loglikelihood is 28.77045559779568\n",
      "likelihood per trial: 0.7263874499726041\n",
      "done running this simulation\n",
      "beginning simulation number 60\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 10.977162797687551\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 10.998157429786493\n",
      "loglikelihood is 29.967396976115243\n",
      "likelihood per trial: 0.7167909252325532\n",
      "done running this simulation\n",
      "beginning simulation number 61\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 15.237883271875846\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 12.916705909424614\n",
      "loglikelihood is 30.674437431032047\n",
      "likelihood per trial: 0.7111818733186118\n",
      "done running this simulation\n",
      "beginning simulation number 62\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 24.707711950524867\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 43.93389178024403\n",
      "loglikelihood is 26.84541209745679\n",
      "likelihood per trial: 0.7420917756240999\n",
      "done running this simulation\n",
      "beginning simulation number 63\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 34.06684871110767\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 25.582263165988735\n",
      "loglikelihood is 26.747502063896043\n",
      "likelihood per trial: 0.7428995285912858\n",
      "done running this simulation\n",
      "beginning simulation number 64\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 36.48821050874322\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 24.052560484971497\n",
      "likelihood per trial: 0.7654811626769988\n",
      "done running this simulation\n",
      "beginning simulation number 65\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 19.356162778541616\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 11.276840873834349\n",
      "loglikelihood is 32.3008976007998\n",
      "likelihood per trial: 0.6984449869472231\n",
      "done running this simulation\n",
      "beginning simulation number 66\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 11.199691180382844\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 8.294969374418272\n",
      "loglikelihood is 30.54230886322214\n",
      "likelihood per trial: 0.7122267227932398\n",
      "done running this simulation\n",
      "beginning simulation number 67\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 22.988013749135924\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 25.828183881547222\n",
      "loglikelihood is 30.203460275752143\n",
      "likelihood per trial: 0.7149132995123655\n",
      "done running this simulation\n",
      "beginning simulation number 68\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 8.578612160902408\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 12.990656366887404\n",
      "loglikelihood is 27.864368992228247\n",
      "likelihood per trial: 0.7337373854242729\n",
      "done running this simulation\n",
      "beginning simulation number 69\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 35.886935164032444\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 37.304531925919015\n",
      "loglikelihood is 28.52620837108724\n",
      "likelihood per trial: 0.7283614397714926\n",
      "done running this simulation\n",
      "beginning simulation number 70\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 2.0037307245056493\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 2.560190458191964\n",
      "loglikelihood is 52.59808028938176\n",
      "likelihood per trial: 0.557427340921927\n",
      "done running this simulation\n",
      "beginning simulation number 71\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 32.19998520265666\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 24.966289672594442\n",
      "likelihood per trial: 0.7577488970652478\n",
      "done running this simulation\n",
      "beginning simulation number 72\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 30.458492018461893\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 22.660750664621634\n",
      "loglikelihood is 24.60482396756983\n",
      "likelihood per trial: 0.7607983527008542\n",
      "done running this simulation\n",
      "beginning simulation number 73\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 6.100161102739301\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 5.448002126041383\n",
      "loglikelihood is 38.063610596090605\n",
      "likelihood per trial: 0.6551251410068781\n",
      "done running this simulation\n",
      "beginning simulation number 74\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 48.43496366437881\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 27.41307669037621\n",
      "likelihood per trial: 0.7374258479290033\n",
      "done running this simulation\n",
      "beginning simulation number 75\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 25.51850885903811\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 25.38065898749056\n",
      "likelihood per trial: 0.7542681616986131\n",
      "done running this simulation\n",
      "beginning simulation number 76\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 33.83760284540739\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 25.85297067334734\n",
      "likelihood per trial: 0.7503202003936876\n",
      "done running this simulation\n",
      "beginning simulation number 77\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 8.09026207420127\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 8.528510352856026\n",
      "loglikelihood is 39.67234919052361\n",
      "likelihood per trial: 0.6435189011925607\n",
      "done running this simulation\n",
      "beginning simulation number 78\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 14.45841709401849\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 10.90319906231814\n",
      "loglikelihood is 31.776933525265093\n",
      "likelihood per trial: 0.7025230694985314\n",
      "done running this simulation\n",
      "beginning simulation number 79\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 34.79331865246523\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 36.59768951133643\n",
      "loglikelihood is 26.548409298939276\n",
      "likelihood per trial: 0.7445447467731555\n",
      "done running this simulation\n",
      "beginning simulation number 80\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 42.39493209159224\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 46.914427789892564\n",
      "loglikelihood is 31.211531633031374\n",
      "likelihood per trial: 0.7069503824754061\n",
      "done running this simulation\n",
      "beginning simulation number 81\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 25.792060932988615\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 29.695438742921947\n",
      "loglikelihood is 33.63463613834493\n",
      "likelihood per trial: 0.6881708252767009\n",
      "done running this simulation\n",
      "beginning simulation number 82\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 39.19563392884216\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 30.460121782759675\n",
      "loglikelihood is 29.57197516040762\n",
      "likelihood per trial: 0.7199471288779192\n",
      "done running this simulation\n",
      "beginning simulation number 83\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 31.024938452691682\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 24.732706472534602\n",
      "likelihood per trial: 0.7597180892635059\n",
      "done running this simulation\n",
      "beginning simulation number 84\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 48.89848120905173\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 24.15032786884034\n",
      "likelihood per trial: 0.7646500687172302\n",
      "done running this simulation\n",
      "beginning simulation number 85\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 36.61490694445937\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 20.811861549632987\n",
      "loglikelihood is 29.2504893479571\n",
      "likelihood per trial: 0.7225234251273694\n",
      "done running this simulation\n",
      "beginning simulation number 86\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 6.895074173646261\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 3.810004534618258\n",
      "loglikelihood is 46.07459495843828\n",
      "likelihood per trial: 0.5993317802445417\n",
      "done running this simulation\n",
      "beginning simulation number 87\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 47.83951288980968\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 25.75677669053319\n",
      "likelihood per trial: 0.7511225878827009\n",
      "done running this simulation\n",
      "beginning simulation number 88\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 32.500181829432705\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 23.913198988119188\n",
      "likelihood per trial: 0.7666673986450089\n",
      "done running this simulation\n",
      "beginning simulation number 89\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 44.18184793784552\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 25.99576194162943\n",
      "likelihood per trial: 0.7491307089927701\n",
      "done running this simulation\n",
      "beginning simulation number 90\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 22.025690554639038\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 16.364928963630092\n",
      "loglikelihood is 27.404374115383373\n",
      "likelihood per trial: 0.737497156973719\n",
      "done running this simulation\n",
      "beginning simulation number 91\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 10.149714663411833\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 9.836420929748126\n",
      "loglikelihood is 31.033417102859502\n",
      "likelihood per trial: 0.7083508582157393\n",
      "done running this simulation\n",
      "beginning simulation number 92\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 24.888173143916124\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 25.060625350275167\n",
      "likelihood per trial: 0.7569550603362218\n",
      "done running this simulation\n",
      "beginning simulation number 93\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 18.351144957143777\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 22.13742033310586\n",
      "loglikelihood is 32.552462927650005\n",
      "likelihood per trial: 0.6964954401915936\n",
      "done running this simulation\n",
      "beginning simulation number 94\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 11.997127109020672\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 8.411241344900226\n",
      "loglikelihood is 33.605684661936905\n",
      "likelihood per trial: 0.6883922337910625\n",
      "done running this simulation\n",
      "beginning simulation number 95\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 18.066764264942968\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 30.03201301967205\n",
      "loglikelihood is 26.988840391748866\n",
      "likelihood per trial: 0.7409100846117207\n",
      "done running this simulation\n",
      "beginning simulation number 96\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 11.802692106285742\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 17.86175593934756\n",
      "loglikelihood is 27.58960411241573\n",
      "likelihood per trial: 0.7359808667869\n",
      "done running this simulation\n",
      "beginning simulation number 97\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 21.59687816561818\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 31.385371409437717\n",
      "loglikelihood is 28.1845846463535\n",
      "likelihood per trial: 0.7311314219324876\n",
      "done running this simulation\n",
      "beginning simulation number 98\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 28.94304560137716\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 23.276307463225724\n",
      "likelihood per trial: 0.7721120179744762\n",
      "done running this simulation\n",
      "beginning simulation number 99\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 32.1181619441015\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 27.588977552793803\n",
      "loglikelihood is 28.543088902322054\n",
      "likelihood per trial: 0.7282248400486002\n",
      "done running this simulation\n",
      "beginning simulation number 100\n",
      "alpha for this simulation is: 0.21\n",
      "beta for this simulation is, 14.797518121385874\n",
      "best action = 1\n",
      "Checking simulated dataset...\n",
      "fit beta is: 50.0\n",
      "loglikelihood is 23.46091612063405\n",
      "likelihood per trial: 0.7705298793651532\n",
      "done running this simulation\n"
     ]
    }
   ],
   "source": [
    "# define parameters \n",
    "r_p = np.full((3, 8), 1, dtype=int)\n",
    "r_p = np.append(r_p, [0, 0, 0, 0, 0, 0])\n",
    "r_p = np.random.permutation(r_p)\n",
    "\n",
    "p_p = np.full((3, 8), 0, dtype=int)\n",
    "p_p = np.append(p_p, [-1, -1, -1, -1, -1, -1])\n",
    "p_p = np.random.permutation(p_p)\n",
    "\n",
    "inv_rp = np.full((24), 0, dtype=int)\n",
    "inv_rp = np.append(inv_rp, [1, 1, 1, 1, 1, 1])\n",
    "inv_rp= np.random.permutation(inv_rp)\n",
    "\n",
    "inv_pp = np.full((24), -1, dtype=int)\n",
    "inv_pp = np.append(inv_pp, [0, 0, 0, 0, 0, 0])\n",
    "inv_pp = np.random.permutation(inv_pp)\n",
    "\n",
    "n_timesteps = 3 \n",
    "n_trials_per_block = 30 \n",
    "\n",
    "best_action = np.random.choice(2) + 1   \n",
    "\n",
    "### Initialize output list to simulate through different values of alpha\n",
    "D = []\n",
    "\n",
    "for i in np.arange(1,101): # number of simulations to run\n",
    "    print('beginning simulation number ' + str(i))\n",
    "    \n",
    "    alpha = 0.21\n",
    "    print('alpha for this simulation is: ' + str(alpha))\n",
    "\n",
    "    for beta in np.random.uniform(1,50,1):\n",
    "        print('beta for this simulation is, ' + str(beta))\n",
    "        print('best action = ' + str(best_action))\n",
    "\n",
    "        for i in np.arange(n_timesteps):\n",
    "\n",
    "            params = {\n",
    "            'n_actions' : 2,\n",
    "            'r_p': r_p,\n",
    "            'p_p': p_p,\n",
    "            'inv_rp': inv_rp,\n",
    "            'inv_pp': inv_pp,\n",
    "            'best_action' : best_action,\n",
    "            'alpha' : alpha, \n",
    "            'beta' : beta  \n",
    "            }\n",
    "\n",
    "        _, _, sim_output = RP_simulation(n_timesteps, n_trials_per_block, params) \n",
    "\n",
    "        # Convert to dataframe and append alpha, beta, rewards, & optimal action    \n",
    "        d=pd.DataFrame(sim_output['actions'], columns = ['actions'])\n",
    "        d.insert(1, 'alpha', alpha),\n",
    "        d.insert(2, 'beta', beta),\n",
    "        d.insert(3, 'rewards', sim_output['rewards']),\n",
    "        d.insert(4, 'condition', sim_output['condition'])\n",
    "        d.insert(5, 'optimal_action', sim_output['optimal_action']),\n",
    "        d.insert(6, 'best_action', best_action),\n",
    "        \n",
    "        ### Then, fit simulated dataset to recover parameters. \n",
    "        \n",
    "        print('Checking simulated dataset...')\n",
    "\n",
    "        # Obtain Log Likelihood function parameters (alpha, beta, actions, rewards) using d   \n",
    "        alpha = d[\"alpha\"].iloc[0]\n",
    "\n",
    "        sim_beta = d[\"beta\"].iloc[0]\n",
    "    \n",
    "        actions = d['actions']\n",
    "        \n",
    "        reward = d['rewards']\n",
    "\n",
    "        condition = d[\"condition\"]\n",
    "\n",
    "        # Then, compute best fit alpha which comes from fitting function\n",
    "        fit_beta = fit_RP_Learning(alpha, actions, reward, condition)\n",
    "        print('fit beta is: ' + str(fit_beta.x[0]))\n",
    "\n",
    "        # max LL gets computed with best fit parameters\n",
    "        max_LL = m1_loglikelihood(fit_beta.x[0], alpha, actions, reward, condition) # this is already returned as a negative in my fx\n",
    "        print('loglikelihood is ' + str(max_LL))\n",
    "\n",
    "        LL_per_trial = np.exp(-max_LL/len(actions))\n",
    "        print('likelihood per trial: ' + str(LL_per_trial))\n",
    "    \n",
    "        print('done running this simulation')\n",
    "\n",
    "        BIC_model1 = model1_BIC(actions, reward, condition) \n",
    "        BIC_model2 = model2_BIC(actions, reward, condition)\n",
    "\n",
    "        d.insert(7, 'log_likelihood', -max_LL),\n",
    "        d.insert(8, 'fit_beta', fit_beta.x[0]),\n",
    "        d.insert(9, 'LL_per_trial', LL_per_trial)\n",
    "        d.insert(10, 'model1_BIC', BIC_model1)\n",
    "        d.insert(11, 'model2_BIC', BIC_model2)\n",
    "\n",
    "#         print('likelihood per trial: ' + str(np.exp(-max_LL/len(actions)))) # likelihood per trial\n",
    "\n",
    "        D.append(d)\n",
    "\n",
    "        data = pd.concat(D, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba1c468c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actions</th>\n",
       "      <th>alpha</th>\n",
       "      <th>beta</th>\n",
       "      <th>rewards</th>\n",
       "      <th>condition</th>\n",
       "      <th>optimal_action</th>\n",
       "      <th>best_action</th>\n",
       "      <th>log_likelihood</th>\n",
       "      <th>fit_beta</th>\n",
       "      <th>LL_per_trial</th>\n",
       "      <th>model1_BIC</th>\n",
       "      <th>model2_BIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>38.512808</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-24.100314</td>\n",
       "      <td>40.448147</td>\n",
       "      <td>0.765075</td>\n",
       "      <td>52.700437</td>\n",
       "      <td>56.947321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>38.512808</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-24.100314</td>\n",
       "      <td>40.448147</td>\n",
       "      <td>0.765075</td>\n",
       "      <td>52.700437</td>\n",
       "      <td>56.947321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>38.512808</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-24.100314</td>\n",
       "      <td>40.448147</td>\n",
       "      <td>0.765075</td>\n",
       "      <td>52.700437</td>\n",
       "      <td>56.947321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>38.512808</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-24.100314</td>\n",
       "      <td>40.448147</td>\n",
       "      <td>0.765075</td>\n",
       "      <td>52.700437</td>\n",
       "      <td>56.947321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>38.512808</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-24.100314</td>\n",
       "      <td>40.448147</td>\n",
       "      <td>0.765075</td>\n",
       "      <td>52.700437</td>\n",
       "      <td>56.947321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>14.797518</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-23.460916</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.770530</td>\n",
       "      <td>51.421642</td>\n",
       "      <td>58.379548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>14.797518</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-23.460916</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.770530</td>\n",
       "      <td>51.421642</td>\n",
       "      <td>58.379548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>14.797518</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-23.460916</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.770530</td>\n",
       "      <td>51.421642</td>\n",
       "      <td>58.379548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>14.797518</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-23.460916</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.770530</td>\n",
       "      <td>51.421642</td>\n",
       "      <td>58.379548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>14.797518</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-23.460916</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.770530</td>\n",
       "      <td>51.421642</td>\n",
       "      <td>58.379548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      actions  alpha       beta  rewards  condition  optimal_action  \\\n",
       "0           2   0.21  38.512808       -1          2               0   \n",
       "1           1   0.21  38.512808        0          2               1   \n",
       "2           1   0.21  38.512808        0          3               3   \n",
       "3           2   0.21  38.512808        0          3               3   \n",
       "4           1   0.21  38.512808        1          1               1   \n",
       "...       ...    ...        ...      ...        ...             ...   \n",
       "8995        1   0.21  14.797518        1          1               1   \n",
       "8996        2   0.21  14.797518        0          3               3   \n",
       "8997        2   0.21  14.797518        0          3               3   \n",
       "8998        1   0.21  14.797518        0          2               1   \n",
       "8999        1   0.21  14.797518        0          3               3   \n",
       "\n",
       "      best_action  log_likelihood   fit_beta  LL_per_trial  model1_BIC  \\\n",
       "0               1      -24.100314  40.448147      0.765075   52.700437   \n",
       "1               1      -24.100314  40.448147      0.765075   52.700437   \n",
       "2               1      -24.100314  40.448147      0.765075   52.700437   \n",
       "3               1      -24.100314  40.448147      0.765075   52.700437   \n",
       "4               1      -24.100314  40.448147      0.765075   52.700437   \n",
       "...           ...             ...        ...           ...         ...   \n",
       "8995            1      -23.460916  50.000000      0.770530   51.421642   \n",
       "8996            1      -23.460916  50.000000      0.770530   51.421642   \n",
       "8997            1      -23.460916  50.000000      0.770530   51.421642   \n",
       "8998            1      -23.460916  50.000000      0.770530   51.421642   \n",
       "8999            1      -23.460916  50.000000      0.770530   51.421642   \n",
       "\n",
       "      model2_BIC  \n",
       "0      56.947321  \n",
       "1      56.947321  \n",
       "2      56.947321  \n",
       "3      56.947321  \n",
       "4      56.947321  \n",
       "...          ...  \n",
       "8995   58.379548  \n",
       "8996   58.379548  \n",
       "8997   58.379548  \n",
       "8998   58.379548  \n",
       "8999   58.379548  \n",
       "\n",
       "[9000 rows x 12 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb4625",
   "metadata": {},
   "source": [
    "## Visualizing Parameter Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08624e89",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7597367102547781"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"white\")\n",
    "sns.despine()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = ( 10 , 5 ))\n",
    "\n",
    "figure = sns.scatterplot(ax = ax, x = \"beta\", y = \"fit_beta\",\n",
    "                data = data, color = \"c\")\n",
    "\n",
    "# Set label for x-axis\n",
    "ax.set_xlabel( \"Simulated Beta\" , size = 20 )\n",
    "  \n",
    "# Set label for y-axis\n",
    "ax.set_ylabel( \"Fit Beta\" , size = 20 )\n",
    "  \n",
    "# Set title for plot\n",
    "ax.set_title( \"Model 1 Simulations\" , size = 20 )\n",
    "  \n",
    "# Display figure\n",
    "plt.show()\n",
    "# sns.despine()\n",
    "data['beta'].corr(data['fit_beta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046342f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
