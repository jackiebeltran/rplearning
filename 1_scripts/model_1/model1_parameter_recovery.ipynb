{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0638f3a",
   "metadata": {},
   "source": [
    "this model updates q values for only reward/punishment trials and operates using a single LR. We perform parameter recovery while holding beta constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58184c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9773607",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    \n",
    "    \"\"\"Class for the RP learning task\n",
    "\n",
    "    Parameters \n",
    "    ----------\n",
    "\n",
    "    n_actions : array, float \n",
    "        choosing the top or bottom stimulus\n",
    "\n",
    "    r_p :\n",
    "        reward probability with 80/20 contingency\n",
    "\n",
    "    p_p :\n",
    "        punishment probability with 80/20 contingency\n",
    "\n",
    "    inv_rp :\n",
    "        inverse reward probability \n",
    "\n",
    "    inv_pp :\n",
    "        inverse punishment probability\n",
    "\n",
    "    best_action : \n",
    "        pre-defined action that's the 'best'\n",
    "        set as a np.random variable that's either 1 or 2 to randomize every time I initialize\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_actions, r_p, p_p, inv_rp, inv_pp, best_action):\n",
    "        \n",
    "        self.n_actions = n_actions # choice of top or bottom stimulus\n",
    "\n",
    "        self.r_p = r_p             # reward prob outcome\n",
    "        self.p_p = p_p             # punishment prob outcome\n",
    "\n",
    "        self.inv_rp = inv_rp       # inverse reward prob\n",
    "        self.inv_pp = inv_pp       # inverse punishment prob\n",
    "\n",
    "        self.best_action = best_action  # predefined best action\n",
    "\n",
    "# Step Function: based on the condition, the environment returns an appropriate reward\n",
    "\n",
    "    \"\"\"\n",
    "    Conditions are set such that 1 = Reward, 2=Punishment, 3=Neutral\n",
    "    The best action is that which is associated with a higher probability of returning reward\n",
    "        - Taking the best action returns a value of 1\n",
    "        - Not taking the best action returns a value of 0 \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def step(self, action, condition, trial): # takes in condition  \n",
    "        \n",
    "        if condition == 1:                          ## Reward\n",
    "            if action == self.best_action:                               \n",
    "                reward = self.r_p[trial]            # index through r_p for 80% chance reward\n",
    "                took_best_action = 1                # true, best action was taken \n",
    "            else: \n",
    "                reward = self.inv_rp[trial]         # index through inv_rp for 20% chance reward\n",
    "                took_best_action = 0                # false \n",
    "                \n",
    "        elif condition == 2:                        ## Punishment\n",
    "            if action == self.best_action:\n",
    "                reward = self.p_p[trial]\n",
    "                took_best_action = 1                \n",
    "            else:\n",
    "                reward = self.inv_pp[trial] \n",
    "                took_best_action = 0\n",
    "        else:                                       ## Neutral\n",
    "            reward = 0\n",
    "            took_best_action = 3        \n",
    "\n",
    "        return reward, took_best_action, condition             # return condition agent's in to update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b943b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \"\"\" Class for the agent to operate on a soft max policy when choosing between the two actions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float, range (0, 1)\n",
    "        Learning rate \n",
    "    beta : float, range (0, inf) \n",
    "      inverse temperature to control level of stochasticity in the choice\n",
    "      **0 means the agent explores randomly \n",
    "      **large value approaching inf acts more deterministically\n",
    "      \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, alpha, beta, q_init= False):\n",
    "\n",
    "        # initialize action space which is an array of all possible actions\n",
    "        self.action_space = np.arange(env.n_actions) # 2 possible actions\n",
    "\n",
    "        # initialize parameters\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "        # initialize Q-values \n",
    "        if q_init: \n",
    "            self.q = q_init # assigns q to those initial values     \n",
    "        else:   \n",
    "            self.q = np.zeros((3, env.n_actions)) # otherwise they are 6 values of 0 (2 stimuli per condition)\n",
    "\n",
    "        # initialize action counter, this counts how many times an action is taken\n",
    "        self.action_counter = np.zeros((env.n_actions, ))\n",
    "        \n",
    "    # Learning policy\n",
    "        \n",
    "    def soft_max_policy(self, condition):        \n",
    "        p = np.exp(self.beta * self.q[condition-1,:]) / (np.exp(self.beta * self.q[condition-1,:])).sum() # prob of choosing an action by condition        \n",
    "        action = np.nonzero(np.random.random((1,)) <= np.cumsum(p))[0][0] + 1               \n",
    "        return action # returns 1 or 2\n",
    "    \n",
    "    # Q-learning update function by trial type (only reward or punishment)\n",
    "  \n",
    "    def update(self, condition, action, reward, verbose=False):\n",
    "        if condition == 1 or condition == 2:\n",
    "            self.action_counter[action-1] = self.action_counter[action-1] + 1        \n",
    "            self.q[condition-1, action-1] = self.q[condition-1, action-1] + self.alpha*(reward - self.q[condition-1, action-1]) # update by condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2265d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RP_simulation(n_timesteps, n_trials_per_block, params, verbose=False):\n",
    "    \n",
    "    \"\"\"Function for running one simulation of the RL model \n",
    "    specifying how the environment and agent interact \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    n_timesteps :\n",
    "        how many timesteps to run the simulation for\n",
    "        \n",
    "    params : dictionary containing parameters of the simulation \n",
    "        \n",
    "        Environment parameters\n",
    "        ----------------------\n",
    "        n_actions: int \n",
    "            number of actions the agent can choose from \n",
    "        r_p : \n",
    "            possible returns from reward condition\n",
    "        p_p : \n",
    "            possible returns from punishment condition\n",
    "        inv_rp :\n",
    "            inverse reward probability\n",
    "        inv_pp :\n",
    "            inverse punishment probability\n",
    "        best_action: int\n",
    "            which is the best action\n",
    "            \n",
    "        Agent parameters\n",
    "        ----------------\n",
    "        alpha : \n",
    "            learning rate \n",
    "        beta : \n",
    "            inverse temperature\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    sim_output: dictionary containing simulation output\n",
    "\n",
    "        actions: array, int, shape(n_timesteps, )\n",
    "            Action that the agent took on each timestep.\n",
    "\n",
    "        rewards: array, float, shape(n_timesteps, )\n",
    "            Rewards that the agent received on each timestep.\n",
    "\n",
    "        optimal_action: array, boolean, shape(n_timesteps, )\n",
    "            1 is true, 0 is false\n",
    "            \n",
    "        condition: \n",
    "            reward(1), punishment(2), neutral(3)\n",
    "        \n",
    "    \"\"\"\n",
    "    # initialize environment \n",
    "    env = Environment(params['n_actions'], params['r_p'], params['p_p'], params['inv_rp'], params['inv_pp'], params['best_action'])\n",
    "    \n",
    "    # initialize agent\n",
    "    agent = Agent(env, params['alpha'], params['beta'])\n",
    "    \n",
    "    # initialize output lists \n",
    "    A = [] # action taken \n",
    "    R = [] # reward taken\n",
    "    OA = [] # was optimal action taken \n",
    "    C = [] # condition\n",
    "    \n",
    "    # Loop through trials\n",
    "    \n",
    "    a = np.tile([1], 30) # reward\n",
    "    b = np.tile([2], 30) # punishment \n",
    "    c = np.tile([3], 30) # neutral\n",
    "    d = np.concatenate([a,b,c])\n",
    "\n",
    "    np.random.shuffle(d) # shuffle order of conditions\n",
    "    e = np.array_split(d,3)\n",
    "        \n",
    "    for i in np.arange(n_timesteps): # 3\n",
    "        for t in np.arange(n_trials_per_block): # 30\n",
    "            \n",
    "            condition = e[i][t]       \n",
    "        \n",
    "            # agent takes an action based on soft max policy\n",
    "            action = agent.soft_max_policy(condition) \n",
    "            \n",
    "            # environment responds with a reward \n",
    "            reward, took_best_action, condition = env.step(action, condition, t)\n",
    "\n",
    "            # record action, reward, and optimal outcome result\n",
    "            A.append(action)\n",
    "            R.append(reward)\n",
    "            OA.append(took_best_action)\n",
    "            C.append(condition)\n",
    "\n",
    "            # update \n",
    "            agent.update(condition, action, reward)\n",
    "        \n",
    "    sim_output = {\n",
    "        'timestep': np.arange(n_timesteps)+1,\n",
    "        'actions': np.array(A),\n",
    "        'rewards': np.array(R),\n",
    "        'optimal_action': np.array(OA),\n",
    "        'condition': np.array(C)\n",
    "    }\n",
    "        \n",
    "    return env, agent, sim_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d70e1e9",
   "metadata": {},
   "source": [
    "## Parameter Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dedbdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T is the number of trials, 90 \n",
    "# L represents the log likelihood for all trials such that as you go through each trial you compute one value and that gets added on\n",
    "\n",
    "def m1_loglikelihood(alpha, beta, actions, reward, condition):\n",
    "    \n",
    "    q_value = np.ones((3,2))*0.0\n",
    "    T = len(actions)\n",
    "    L = 0  \n",
    "    \n",
    "    for t in range(T): # for every trial \n",
    "        \n",
    "        # compute choice probabilities of picking an action based on soft max \n",
    "        p = np.exp(beta * q_value[condition[t]-1,:]) / (np.exp(beta * q_value[condition[t]-1,:])).sum()\n",
    "\n",
    "        # compute choice probability for actual choice based on the probability computed above by condition \n",
    "        choiceProb = p[actions[t]-1]\n",
    " \n",
    "        # sum of the natural log of each individual choice probability to get the prob of a whole dataset (90 trials)\n",
    "        L += np.log(choiceProb) \n",
    "        \n",
    "        # update values with q learning, index for t and update by condition \n",
    "        if condition[t] == 1 or condition[t] == 2:\n",
    "            q_value[condition[t]-1, actions[t]-1] = q_value[condition[t]-1, actions[t]-1] + alpha * (reward[t] - q_value[condition[t]-1, actions[t]-1])\n",
    "\n",
    "    return -L   # this will return a negative log likelihood which is what we want to minimize to perform parameter recovery/fitting on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "017ffe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fitting alpha\n",
    "This fitting function returns a value for 'fun' which is the -LL & a value for 'x' which is the best fit alpha\n",
    "\n",
    "'''\n",
    "\n",
    "def fit_RP_Learning(beta, actions, rewards, condition):\n",
    "    \n",
    "    init_cond = [np.random.uniform(0,1)]\n",
    "    \n",
    "    bnds = [(1e-6,1)]\n",
    "        \n",
    "    optimum_output = minimize(m1_loglikelihood, init_cond, args=(beta, actions, rewards, condition), method='L-BFGS-B',bounds=bnds)\n",
    "    \n",
    "    return optimum_output "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40725fe0",
   "metadata": {},
   "source": [
    "### Model Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaa581bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1_BIC(actions, rewards, condition):\n",
    "    \n",
    "    init_cond = [np.random.uniform(0,1)] # learning rate\n",
    "    bnds = [(1e-6,1)]\n",
    "    km = len(bnds) # number of parameters fit in the model\n",
    "    \n",
    "    optimum_output = minimize(m1_loglikelihood, init_cond, args=(beta, actions, rewards, condition), method='L-BFGS-B',bounds=bnds)    \n",
    "    neg_loglikelihood = optimum_output.fun\n",
    "    \n",
    "    BIC = km * np.log(len(actions)) + 2*neg_loglikelihood\n",
    "    return BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef17fca",
   "metadata": {},
   "source": [
    "##### load model 2 functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28d9d917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2_BIC(actions, rewards, condition):\n",
    "    \n",
    "    init_cond = np.array([np.random.uniform(0,1), # reward alpha\n",
    "                          np.random.uniform(0,1)]) # punishment alpha\n",
    "    bnds = [(1e-6,1), (1e-6,1)] \n",
    "    km = len(bnds) # number of parameters fit in the model\n",
    "\n",
    "    optimum_output = minimize(m2_loglikelihood, init_cond, args=(beta, actions, rewards, condition), method='L-BFGS-B', bounds=bnds)\n",
    "    \n",
    "    neg_loglikelihood = optimum_output.fun\n",
    "    \n",
    "    BIC = km * np.log(len(actions)) + 2*neg_loglikelihood\n",
    "    \n",
    "    return BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "127a1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m2_loglikelihood(training_params, beta, actions, rewards, condition):\n",
    "    \n",
    "    r_alpha = training_params[0]\n",
    "    p_alpha = training_params[1]\n",
    "\n",
    "    q_value = np.ones((3,2))*0.0  \n",
    "    T = len(actions)\n",
    "    L = 0  \n",
    "    \n",
    "    for t in range(T): # for every trial        \n",
    "        \n",
    "        # compute choice probabilities of picking an action based on soft max \n",
    "        p = np.exp(beta * q_value[condition[t]-1,:]) / (np.exp(beta * q_value[condition[t]-1,:])).sum()\n",
    "        #print('LL function probability is ' + str(p))\n",
    "\n",
    "        # compute choice probability for actual choice based on the probability computed above by condition \n",
    "        choiceProb = p[actions[t]-1]\n",
    "        #print('choice probability is ' + str(choiceProb))\n",
    "\n",
    "        # sum of the natural log of each individual choice probability to get the prob of a whole dataset (90 trials)\n",
    "        L += np.log(choiceProb) \n",
    "    \n",
    "        # update values with q learning, index for t and update by condition using the two separate learning rates\n",
    "        if condition[t] == 1:\n",
    "            q_value[condition[t]-1, actions[t]-1] = q_value[condition[t]-1, actions[t]-1] + r_alpha * (rewards[t] - q_value[condition[t]-1, actions[t]-1])\n",
    "        elif condition[t] == 2:\n",
    "            q_value[condition[t]-1, actions[t]-1] = q_value[condition[t]-1, actions[t]-1] + p_alpha * (rewards[t] - q_value[condition[t]-1, actions[t]-1])\n",
    "    \n",
    "    return -L   # this will return a negative log likelihood which is what we want to minimize to perform parameter recovery/fitting on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004a0ce5",
   "metadata": {},
   "source": [
    "## Running a Simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06a72205",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning simulation number 1\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.8063210055255228\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.8969019902726102\n",
      "likelihood per trial: 0.6758537825269662\n",
      "done running this simulation\n",
      "beginning simulation number 2\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.7764142181047976\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.7015617979392941\n",
      "likelihood per trial: 0.6699237995629954\n",
      "done running this simulation\n",
      "beginning simulation number 3\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.910715736996869\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.7327652674555192\n",
      "likelihood per trial: 0.5983662833671244\n",
      "done running this simulation\n",
      "beginning simulation number 4\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.5748306796451391\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.609369015486686\n",
      "likelihood per trial: 0.6134952228981282\n",
      "done running this simulation\n",
      "beginning simulation number 5\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.09689211387591523\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.13876308702979645\n",
      "likelihood per trial: 0.6177187415721586\n",
      "done running this simulation\n",
      "beginning simulation number 6\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.7912430715606814\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 1.0\n",
      "likelihood per trial: 0.6783673210367676\n",
      "done running this simulation\n",
      "beginning simulation number 7\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.8956577170805853\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.7139759461270606\n",
      "likelihood per trial: 0.6886007003834667\n",
      "done running this simulation\n",
      "beginning simulation number 8\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.6390944273218196\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.8317305931586785\n",
      "likelihood per trial: 0.6941775479466208\n",
      "done running this simulation\n",
      "beginning simulation number 9\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.09155653430010191\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.08478360738411417\n",
      "likelihood per trial: 0.5762477416061715\n",
      "done running this simulation\n",
      "beginning simulation number 10\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.7551326696865609\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.8500959283160392\n",
      "likelihood per trial: 0.6527902753194843\n",
      "done running this simulation\n",
      "beginning simulation number 11\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.946929248774972\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 1.0\n",
      "likelihood per trial: 0.599136932552023\n",
      "done running this simulation\n",
      "beginning simulation number 12\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.46218155671316197\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.5262091796382918\n",
      "likelihood per trial: 0.6936279795596307\n",
      "done running this simulation\n",
      "beginning simulation number 13\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.4496999224383753\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.8457739765733804\n",
      "likelihood per trial: 0.5973800065612886\n",
      "done running this simulation\n",
      "beginning simulation number 14\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.9974227627900848\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.7910682729251407\n",
      "likelihood per trial: 0.6804113332065418\n",
      "done running this simulation\n",
      "beginning simulation number 15\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.26693573891173095\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.1339114811086557\n",
      "likelihood per trial: 0.5949543741072526\n",
      "done running this simulation\n",
      "beginning simulation number 16\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.6438389757017299\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.6764120045558784\n",
      "likelihood per trial: 0.7148393857417479\n",
      "done running this simulation\n",
      "beginning simulation number 17\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.5015291039844454\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.27520917952595036\n",
      "likelihood per trial: 0.6375389148655167\n",
      "done running this simulation\n",
      "beginning simulation number 18\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.6293393841873\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.9691607897010609\n",
      "likelihood per trial: 0.6226263469215566\n",
      "done running this simulation\n",
      "beginning simulation number 19\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.31092018063538607\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.6099128623118864\n",
      "likelihood per trial: 0.6347783848576379\n",
      "done running this simulation\n",
      "beginning simulation number 20\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.06716652137570944\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.06346129891460699\n",
      "likelihood per trial: 0.5450460593221091\n",
      "done running this simulation\n",
      "beginning simulation number 21\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.06257281445031104\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.03686832230171532\n",
      "likelihood per trial: 0.545747348553425\n",
      "done running this simulation\n",
      "beginning simulation number 22\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.016122198995457748\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 1e-06\n",
      "likelihood per trial: 0.49999952775748696\n",
      "done running this simulation\n",
      "beginning simulation number 23\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.026785946672688632\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.03370299486418519\n",
      "likelihood per trial: 0.5152715483124021\n",
      "done running this simulation\n",
      "beginning simulation number 24\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.23870881941393784\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.23173904530206346\n",
      "likelihood per trial: 0.6302197069328083\n",
      "done running this simulation\n",
      "beginning simulation number 25\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.8716575710360945\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.8121779288669533\n",
      "likelihood per trial: 0.6207388704301183\n",
      "done running this simulation\n",
      "beginning simulation number 26\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.353953813785012\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.3065037634651644\n",
      "likelihood per trial: 0.6467658950748748\n",
      "done running this simulation\n",
      "beginning simulation number 27\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.16773864135258232\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.23986592553981614\n",
      "likelihood per trial: 0.6254725359425054\n",
      "done running this simulation\n",
      "beginning simulation number 28\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.6221679452297005\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.27792660312001705\n",
      "likelihood per trial: 0.6232950044560363\n",
      "done running this simulation\n",
      "beginning simulation number 29\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.6796789289458883\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.7565995058031968\n",
      "likelihood per trial: 0.6538831604372666\n",
      "done running this simulation\n",
      "beginning simulation number 30\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.9390240070015476\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.019235130819101415\n",
      "likelihood per trial: 0.5091302620266467\n",
      "done running this simulation\n",
      "beginning simulation number 31\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.8013286457762615\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.5609201528001228\n",
      "likelihood per trial: 0.7282863668309387\n",
      "done running this simulation\n",
      "beginning simulation number 32\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.8511524365039418\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 1.0\n",
      "likelihood per trial: 0.6333642743112858\n",
      "done running this simulation\n",
      "beginning simulation number 33\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.7294313909047108\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.7415335334856153\n",
      "likelihood per trial: 0.6233916372351046\n",
      "done running this simulation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning simulation number 34\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.13205609559878162\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.06460151934269145\n",
      "likelihood per trial: 0.5351090393208613\n",
      "done running this simulation\n",
      "beginning simulation number 35\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.8712646017502637\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.8171269435768427\n",
      "likelihood per trial: 0.6194901186873522\n",
      "done running this simulation\n",
      "beginning simulation number 36\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.8240223309027183\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.9419657935991879\n",
      "likelihood per trial: 0.6488462154854694\n",
      "done running this simulation\n",
      "beginning simulation number 37\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.3152209634237617\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.2886076379191993\n",
      "likelihood per trial: 0.6641873861752059\n",
      "done running this simulation\n",
      "beginning simulation number 38\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.9490494686875014\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.8097229883210811\n",
      "likelihood per trial: 0.6445942814564989\n",
      "done running this simulation\n",
      "beginning simulation number 39\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.7878859576320388\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.3983579933098265\n",
      "likelihood per trial: 0.7005525092168889\n",
      "done running this simulation\n",
      "beginning simulation number 40\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.3209821571961031\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.5078994499533689\n",
      "likelihood per trial: 0.6909318512526688\n",
      "done running this simulation\n",
      "beginning simulation number 41\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.26855203928862137\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.3074941569950416\n",
      "likelihood per trial: 0.6401895173605786\n",
      "done running this simulation\n",
      "beginning simulation number 42\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.37213555982295454\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.39251389831972\n",
      "likelihood per trial: 0.6315585456668396\n",
      "done running this simulation\n",
      "beginning simulation number 43\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.25686614108932604\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.2187063168667415\n",
      "likelihood per trial: 0.6632532672990564\n",
      "done running this simulation\n",
      "beginning simulation number 44\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.2884204513286337\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.27321732184847847\n",
      "likelihood per trial: 0.5874691870980214\n",
      "done running this simulation\n",
      "beginning simulation number 45\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.16981260487912664\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.16181709235154262\n",
      "likelihood per trial: 0.5821885747453119\n",
      "done running this simulation\n",
      "beginning simulation number 46\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.2545821905221869\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.23132799340769925\n",
      "likelihood per trial: 0.6268905889271551\n",
      "done running this simulation\n",
      "beginning simulation number 47\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.8176299208130353\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.8149136604982522\n",
      "likelihood per trial: 0.6492103286516767\n",
      "done running this simulation\n",
      "beginning simulation number 48\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.7833100372590587\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.6228382470121934\n",
      "likelihood per trial: 0.6452162235935277\n",
      "done running this simulation\n",
      "beginning simulation number 49\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.2129949225674408\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.13841443367205838\n",
      "likelihood per trial: 0.5799908113214771\n",
      "done running this simulation\n",
      "beginning simulation number 50\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.8652162318587824\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.9172827168731231\n",
      "likelihood per trial: 0.675303049896222\n",
      "done running this simulation\n",
      "beginning simulation number 51\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.5372073293903648\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.6007403575443607\n",
      "likelihood per trial: 0.5807169152044186\n",
      "done running this simulation\n",
      "beginning simulation number 52\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.022947051683177788\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 1e-06\n",
      "likelihood per trial: 0.4999999722101744\n",
      "done running this simulation\n",
      "beginning simulation number 53\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.9478146087911095\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.8444332918328009\n",
      "likelihood per trial: 0.6630787996095323\n",
      "done running this simulation\n",
      "beginning simulation number 54\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.014651825848235434\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 1e-06\n",
      "likelihood per trial: 0.49999977773654974\n",
      "done running this simulation\n",
      "beginning simulation number 55\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.6075697080004377\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.5547820869668251\n",
      "likelihood per trial: 0.6944794564442521\n",
      "done running this simulation\n",
      "beginning simulation number 56\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.5621919593614719\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.5255784446509889\n",
      "likelihood per trial: 0.6990592045571072\n",
      "done running this simulation\n",
      "beginning simulation number 57\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.833443551093189\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 1.0\n",
      "likelihood per trial: 0.6579841423418716\n",
      "done running this simulation\n",
      "beginning simulation number 58\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.12574195680678768\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.10977815815893924\n",
      "likelihood per trial: 0.6017498210532344\n",
      "done running this simulation\n",
      "beginning simulation number 59\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.02679854840716489\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.013088699369224629\n",
      "likelihood per trial: 0.5028057804994267\n",
      "done running this simulation\n",
      "beginning simulation number 60\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.23956275836456753\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.3304755661784492\n",
      "likelihood per trial: 0.6703425232564705\n",
      "done running this simulation\n",
      "beginning simulation number 61\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.6294561817226431\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.7500046766769322\n",
      "likelihood per trial: 0.6830342765098143\n",
      "done running this simulation\n",
      "beginning simulation number 62\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.7846285600985485\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.6345305406490599\n",
      "likelihood per trial: 0.6751995548688497\n",
      "done running this simulation\n",
      "beginning simulation number 63\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.172166353747631\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.20190115719621876\n",
      "likelihood per trial: 0.6440459657927053\n",
      "done running this simulation\n",
      "beginning simulation number 64\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.580278107043386\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.4298377759791357\n",
      "likelihood per trial: 0.7125487527003669\n",
      "done running this simulation\n",
      "beginning simulation number 65\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.47368543552387965\n",
      "best action = 2\n",
      "Checking simulated dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit alpha is: 0.47448359961381964\n",
      "likelihood per trial: 0.687708436825138\n",
      "done running this simulation\n",
      "beginning simulation number 66\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.6729727572303005\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.6674332230629759\n",
      "likelihood per trial: 0.6553060271834182\n",
      "done running this simulation\n",
      "beginning simulation number 67\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.7585506179932819\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.827335794831525\n",
      "likelihood per trial: 0.6803697639438749\n",
      "done running this simulation\n",
      "beginning simulation number 68\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.15757530172676237\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.11491169609839037\n",
      "likelihood per trial: 0.5909409680821335\n",
      "done running this simulation\n",
      "beginning simulation number 69\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.9935534737110424\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 1.0\n",
      "likelihood per trial: 0.6579841423418716\n",
      "done running this simulation\n",
      "beginning simulation number 70\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.8751950976307993\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.7039113943558145\n",
      "likelihood per trial: 0.6879193518688926\n",
      "done running this simulation\n",
      "beginning simulation number 71\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.2721752304504593\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.2848509205137462\n",
      "likelihood per trial: 0.62401401435709\n",
      "done running this simulation\n",
      "beginning simulation number 72\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.06571729016429839\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.12767609109874303\n",
      "likelihood per trial: 0.6093950117810858\n",
      "done running this simulation\n",
      "beginning simulation number 73\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.022943142307143005\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.039532971420247635\n",
      "likelihood per trial: 0.5344382193202807\n",
      "done running this simulation\n",
      "beginning simulation number 74\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.4948233420219148\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.6771841014813825\n",
      "likelihood per trial: 0.63575471851707\n",
      "done running this simulation\n",
      "beginning simulation number 75\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.4152494172095679\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.5216155729561329\n",
      "likelihood per trial: 0.6895824704568582\n",
      "done running this simulation\n",
      "beginning simulation number 76\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.06406499449916458\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.07233485722408661\n",
      "likelihood per trial: 0.5447410735270612\n",
      "done running this simulation\n",
      "beginning simulation number 77\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.010614771957403235\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.003039407910525105\n",
      "likelihood per trial: 0.5003719396766431\n",
      "done running this simulation\n",
      "beginning simulation number 78\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.7931823730276207\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.8282374201849789\n",
      "likelihood per trial: 0.6690101896923929\n",
      "done running this simulation\n",
      "beginning simulation number 79\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.349926060555372\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.37098230082608663\n",
      "likelihood per trial: 0.6852571297157715\n",
      "done running this simulation\n",
      "beginning simulation number 80\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.1309944217795691\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.1087620260475294\n",
      "likelihood per trial: 0.5906991687294488\n",
      "done running this simulation\n",
      "beginning simulation number 81\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.7768572501485138\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.38434723693376194\n",
      "likelihood per trial: 0.6326868143914931\n",
      "done running this simulation\n",
      "beginning simulation number 82\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.9637795993246682\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.9320933549411206\n",
      "likelihood per trial: 0.6281899478181375\n",
      "done running this simulation\n",
      "beginning simulation number 83\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.7312328345721932\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.7237976628886343\n",
      "likelihood per trial: 0.6504709668095615\n",
      "done running this simulation\n",
      "beginning simulation number 84\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.6641605782501281\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.4657948052482054\n",
      "likelihood per trial: 0.6827045498123054\n",
      "done running this simulation\n",
      "beginning simulation number 85\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.5633301689187059\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.640372499477931\n",
      "likelihood per trial: 0.6781082901754658\n",
      "done running this simulation\n",
      "beginning simulation number 86\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.8301985277988383\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.6844530714894457\n",
      "likelihood per trial: 0.6689658254776422\n",
      "done running this simulation\n",
      "beginning simulation number 87\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.25640403826036506\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.3040996967766738\n",
      "likelihood per trial: 0.5938550989391194\n",
      "done running this simulation\n",
      "beginning simulation number 88\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.26031413752455157\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.3524638638938542\n",
      "likelihood per trial: 0.6217743666527933\n",
      "done running this simulation\n",
      "beginning simulation number 89\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.394809292306438\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.3632982687624545\n",
      "likelihood per trial: 0.6585004194873214\n",
      "done running this simulation\n",
      "beginning simulation number 90\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.8417403732752743\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.9322172171312204\n",
      "likelihood per trial: 0.674283193275679\n",
      "done running this simulation\n",
      "beginning simulation number 91\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.1648919679907882\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.17947707861079348\n",
      "likelihood per trial: 0.6147065993810946\n",
      "done running this simulation\n",
      "beginning simulation number 92\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.6210455722482084\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.6126528515698629\n",
      "likelihood per trial: 0.6148896873844082\n",
      "done running this simulation\n",
      "beginning simulation number 93\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.2728140262894996\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.3520364474013028\n",
      "likelihood per trial: 0.6865799337095475\n",
      "done running this simulation\n",
      "beginning simulation number 94\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.20885247986563515\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.1685403875960698\n",
      "likelihood per trial: 0.6398583968187429\n",
      "done running this simulation\n",
      "beginning simulation number 95\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.5743223830474269\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.5962356360606363\n",
      "likelihood per trial: 0.701544305221944\n",
      "done running this simulation\n",
      "beginning simulation number 96\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.26223063146845105\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.23456878008322302\n",
      "likelihood per trial: 0.5988693066734638\n",
      "done running this simulation\n",
      "beginning simulation number 97\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.2872135691047465\n",
      "best action = 2\n",
      "Checking simulated dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit alpha is: 0.2632826621466171\n",
      "likelihood per trial: 0.6336660849701665\n",
      "done running this simulation\n",
      "beginning simulation number 98\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.09256198604857357\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.04513176265506234\n",
      "likelihood per trial: 0.5128235349393228\n",
      "done running this simulation\n",
      "beginning simulation number 99\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.3731354933083014\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.35887953047273985\n",
      "likelihood per trial: 0.6743684548300323\n",
      "done running this simulation\n",
      "beginning simulation number 100\n",
      "beta for this simulation is: 5\n",
      "alpha for this simulation is, 0.03754456083498514\n",
      "best action = 2\n",
      "Checking simulated dataset...\n",
      "fit alpha is: 0.04446587016427045\n",
      "likelihood per trial: 0.5369296826103589\n",
      "done running this simulation\n"
     ]
    }
   ],
   "source": [
    "# define parameters \n",
    "r_p = np.full((3, 8), 1, dtype=int)\n",
    "r_p = np.append(r_p, [0, 0, 0, 0, 0, 0])\n",
    "r_p = np.random.permutation(r_p)\n",
    "\n",
    "p_p = np.full((3, 8), 0, dtype=int)\n",
    "p_p = np.append(p_p, [-1, -1, -1, -1, -1, -1])\n",
    "p_p = np.random.permutation(p_p)\n",
    "\n",
    "inv_rp = np.full((24), 0, dtype=int)\n",
    "inv_rp = np.append(inv_rp, [1, 1, 1, 1, 1, 1])\n",
    "inv_rp= np.random.permutation(inv_rp)\n",
    "\n",
    "inv_pp = np.full((24), -1, dtype=int)\n",
    "inv_pp = np.append(inv_pp, [0, 0, 0, 0, 0, 0])\n",
    "inv_pp = np.random.permutation(inv_pp)\n",
    "\n",
    "n_timesteps = 3 \n",
    "n_trials_per_block = 30 \n",
    "\n",
    "best_action = np.random.choice(2) + 1   \n",
    "\n",
    "### Initialize output list to simulate through different values of alpha\n",
    "D = []\n",
    "\n",
    "for i in np.arange(1,101): # number of simulations to run\n",
    "    print('beginning simulation number ' + str(i))\n",
    "    \n",
    "    for b in [5]: \n",
    "        b = 5\n",
    "        print('beta for this simulation is: ' + str(b))\n",
    "    \n",
    "        for a in np.random.uniform(0,1,1):  # draw a value for alpha\n",
    "            print('alpha for this simulation is, ' + str(a))\n",
    "            print('best action = ' + str(best_action))\n",
    "\n",
    "            for i in np.arange(n_timesteps):\n",
    "\n",
    "                params = {\n",
    "                'n_actions' : 2,\n",
    "                'r_p': r_p,\n",
    "                'p_p': p_p,\n",
    "                'inv_rp': inv_rp,\n",
    "                'inv_pp': inv_pp,\n",
    "                'best_action' : best_action,\n",
    "                'alpha' : a, \n",
    "                'beta' : b  \n",
    "                }\n",
    "\n",
    "            _, _, sim_output = RP_simulation(n_timesteps, n_trials_per_block, params) \n",
    "\n",
    "            # Convert to dataframe and append alpha, beta, rewards, & optimal action    \n",
    "            d=pd.DataFrame(sim_output['actions'], columns = ['actions'])\n",
    "            d.insert(1, 'alpha', a),\n",
    "            d.insert(2, 'beta', b),\n",
    "            d.insert(3, 'rewards', sim_output['rewards']),\n",
    "            d.insert(4, 'condition', sim_output['condition'])\n",
    "            d.insert(5, 'optimal_action', sim_output['optimal_action']),\n",
    "            d.insert(6, 'best_action', best_action),\n",
    "            \n",
    "            ### Then, fit simulated dataset to recover parameters. \n",
    "            \n",
    "            print('Checking simulated dataset...')\n",
    "\n",
    "            # Obtain Log Likelihood function parameters (alpha, beta, actions, rewards) using d   \n",
    "            sim_alpha = d[\"alpha\"].iloc[0]\n",
    "\n",
    "            beta = d[\"beta\"].iloc[0]\n",
    "        \n",
    "            actions = d['actions']\n",
    "            \n",
    "            reward = d['rewards']\n",
    "\n",
    "            condition = d[\"condition\"]\n",
    "\n",
    "            # Then, compute best fit alpha which comes from fitting function\n",
    "            fit_alpha = fit_RP_Learning(beta, actions, reward, condition)\n",
    "            print('fit alpha is: ' + str(fit_alpha.x[0]))\n",
    "\n",
    "            # max LL gets computed with best fit parameters\n",
    "            max_LL = m1_loglikelihood(fit_alpha.x[0], beta, actions, reward, condition) # this is already returned as a negative in my fx\n",
    " \n",
    "            LL_per_trial = np.exp(-max_LL/len(actions))\n",
    "            print('likelihood per trial: ' + str(LL_per_trial))\n",
    "        \n",
    "            print('done running this simulation')\n",
    "\n",
    "            BIC_model1 = model1_BIC(actions, reward, condition) \n",
    "            BIC_model2 = model2_BIC(actions, reward, condition)\n",
    "\n",
    "            d.insert(7, 'log_likelihood', -max_LL),\n",
    "            d.insert(8, 'fit_alpha', fit_alpha.x[0]),\n",
    "            d.insert(9, 'LL_per_trial', LL_per_trial)\n",
    "            d.insert(10, 'model1_BIC', BIC_model1)\n",
    "            d.insert(11, 'model2_BIC', BIC_model2)\n",
    "\n",
    "    #         print('likelihood per trial: ' + str(np.exp(-max_LL/len(actions)))) # likelihood per trial\n",
    "\n",
    "            D.append(d)\n",
    "\n",
    "            data = pd.concat(D, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba1c468c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actions</th>\n",
       "      <th>alpha</th>\n",
       "      <th>beta</th>\n",
       "      <th>rewards</th>\n",
       "      <th>condition</th>\n",
       "      <th>optimal_action</th>\n",
       "      <th>best_action</th>\n",
       "      <th>log_likelihood</th>\n",
       "      <th>fit_alpha</th>\n",
       "      <th>LL_per_trial</th>\n",
       "      <th>model1_BIC</th>\n",
       "      <th>model2_BIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.806321</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-35.260067</td>\n",
       "      <td>0.896902</td>\n",
       "      <td>0.675854</td>\n",
       "      <td>75.019944</td>\n",
       "      <td>79.519750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.806321</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-35.260067</td>\n",
       "      <td>0.896902</td>\n",
       "      <td>0.675854</td>\n",
       "      <td>75.019944</td>\n",
       "      <td>79.519750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.806321</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-35.260067</td>\n",
       "      <td>0.896902</td>\n",
       "      <td>0.675854</td>\n",
       "      <td>75.019944</td>\n",
       "      <td>79.519750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.806321</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-35.260067</td>\n",
       "      <td>0.896902</td>\n",
       "      <td>0.675854</td>\n",
       "      <td>75.019944</td>\n",
       "      <td>79.519750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.806321</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-35.260067</td>\n",
       "      <td>0.896902</td>\n",
       "      <td>0.675854</td>\n",
       "      <td>75.019944</td>\n",
       "      <td>79.519750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>2</td>\n",
       "      <td>0.037545</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-55.969932</td>\n",
       "      <td>0.044466</td>\n",
       "      <td>0.536930</td>\n",
       "      <td>116.439674</td>\n",
       "      <td>120.272121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>2</td>\n",
       "      <td>0.037545</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-55.969932</td>\n",
       "      <td>0.044466</td>\n",
       "      <td>0.536930</td>\n",
       "      <td>116.439674</td>\n",
       "      <td>120.272121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>2</td>\n",
       "      <td>0.037545</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-55.969932</td>\n",
       "      <td>0.044466</td>\n",
       "      <td>0.536930</td>\n",
       "      <td>116.439674</td>\n",
       "      <td>120.272121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>1</td>\n",
       "      <td>0.037545</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-55.969932</td>\n",
       "      <td>0.044466</td>\n",
       "      <td>0.536930</td>\n",
       "      <td>116.439674</td>\n",
       "      <td>120.272121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>2</td>\n",
       "      <td>0.037545</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-55.969932</td>\n",
       "      <td>0.044466</td>\n",
       "      <td>0.536930</td>\n",
       "      <td>116.439674</td>\n",
       "      <td>120.272121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      actions     alpha  beta  rewards  condition  optimal_action  \\\n",
       "0           1  0.806321     5        0          1               0   \n",
       "1           1  0.806321     5        0          1               0   \n",
       "2           2  0.806321     5        1          1               1   \n",
       "3           2  0.806321     5        0          1               1   \n",
       "4           1  0.806321     5        0          1               0   \n",
       "...       ...       ...   ...      ...        ...             ...   \n",
       "8995        2  0.037545     5        1          1               1   \n",
       "8996        2  0.037545     5        0          2               1   \n",
       "8997        2  0.037545     5        0          3               3   \n",
       "8998        1  0.037545     5        0          3               3   \n",
       "8999        2  0.037545     5        1          1               1   \n",
       "\n",
       "      best_action  log_likelihood  fit_alpha  LL_per_trial  model1_BIC  \\\n",
       "0               2      -35.260067   0.896902      0.675854   75.019944   \n",
       "1               2      -35.260067   0.896902      0.675854   75.019944   \n",
       "2               2      -35.260067   0.896902      0.675854   75.019944   \n",
       "3               2      -35.260067   0.896902      0.675854   75.019944   \n",
       "4               2      -35.260067   0.896902      0.675854   75.019944   \n",
       "...           ...             ...        ...           ...         ...   \n",
       "8995            2      -55.969932   0.044466      0.536930  116.439674   \n",
       "8996            2      -55.969932   0.044466      0.536930  116.439674   \n",
       "8997            2      -55.969932   0.044466      0.536930  116.439674   \n",
       "8998            2      -55.969932   0.044466      0.536930  116.439674   \n",
       "8999            2      -55.969932   0.044466      0.536930  116.439674   \n",
       "\n",
       "      model2_BIC  \n",
       "0      79.519750  \n",
       "1      79.519750  \n",
       "2      79.519750  \n",
       "3      79.519750  \n",
       "4      79.519750  \n",
       "...          ...  \n",
       "8995  120.272121  \n",
       "8996  120.272121  \n",
       "8997  120.272121  \n",
       "8998  120.272121  \n",
       "8999  120.272121  \n",
       "\n",
       "[9000 rows x 12 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb4625",
   "metadata": {},
   "source": [
    "## Visualizing Parameter Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08624e89",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8733698794483705"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"white\")\n",
    "sns.despine()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = ( 10 , 5 ))\n",
    "\n",
    "figure = sns.scatterplot(ax = ax, x = \"alpha\", y = \"fit_alpha\",\n",
    "                data = data, color = \"c\")\n",
    "\n",
    "# Set label for x-axis\n",
    "ax.set_xlabel( \"Simulated Alpha\" , size = 20 )\n",
    "  \n",
    "# Set label for y-axis\n",
    "ax.set_ylabel( \"Fit Alpha\" , size = 20 )\n",
    "  \n",
    "# Set title for plot\n",
    "ax.set_title( \"Model 1 Simulations\" , size = 20 )\n",
    "  \n",
    "# Display figure\n",
    "plt.show()\n",
    "# sns.despine()\n",
    "data['alpha'].corr(data['fit_alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046342f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
